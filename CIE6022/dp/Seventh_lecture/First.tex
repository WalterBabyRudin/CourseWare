
\chapter{Infinite Horizon Problem}
\paragraph{Problem Setting}
\begin{enumerate}
\item
The number of stages is infinite, but
\item
The system is stationary, i.e., the system equation, the cost per stage, and the random disturbance does not change from one stage to next.
\end{enumerate}
The objective is to minimize
\[
J_{\pi}(x_0) = \lim_{N\to\infty}\mathbb{E}_{\omega_k,k\ge0}\Bigg\{
\sum_{k=0}^{N-1}\alpha^kg(x_k,\mu_k(x_k),\omega_k)
\Bigg\}
\]

\begin{enumerate}
\item
Stochastic Shortest Path problem:
\[
\alpha=1,
\]
but it is a finite-state system with a \emph{termination} state
\item
Discounted Problem with $\alpha<1$ and bounded cost per stage:
\[
|g(x,\mu,\omega)|\le C,\quad
\alpha<1
\]
\item
Average cost per stage problem:
\[
\alpha=1,\quad
J_\pi(x_\infty) = \lim_{N\to\infty}\frac{1}{N}\mathbb{E}_\omega
\left\{
\sum_{k=0}^Ng(x_k,\mu_k(x_k),\omega_k)
\right\}
\]
\end{enumerate}
The infinite horizon problem is challenging for analysis part, and elegant in solution solving and algorithm design.

\paragraph{Preview}
The key issue is in the relation between the infinite and finite horizon optimal cost-to-go functions.
w.l.o.g., let $\alpha=1$ and let $J_N(x)$ denote the optimal cost for $N$-stage problem, generated after $N$ DP iterations, starting from $J_0(x)\equiv0$:
\[
J_{k+1}(x)=\min_{u\in U(x)}\mathbb{E}_{\omega}\{g(x,u,\omega)+J_k(f(x,u,\omega))\},\quad\forall x
\]
There are typicl results for total cost problems:
\begin{itemize}
\item
Convergence of DP algorithm~(value iteration):
\[
J^*(x) = \lim_{N\to\infty}J_N(x),\quad\forall x
\]
\item
Bellman's equation holds for all $x$:
\[
J^*(x) = \min_{u\in U(x)}\mathbb{E}_{\omega}\{g(x,u,\omega)+J^*(f(x,u,\omega))\}
\]
\item
Optimality condition: if $\mu(x)$ minimizes in the RHS of Bellman equation for each $x$, then the policy $\{\mu,\mu,\dots\}$ is optimal
\end{itemize}
\paragraph{Problem Formulation}
\begin{enumerate}
\item
The underlying system is 
\[
x_{k+1}=\omega_k
\]
where $\omega_k$ denotes the disturbance
\item
Denote $\tilde{g}(i,u,j)$ as the cost for using $u$ at state $i$ and moving to state $j$, and the stage cost $g(x_k,u_k)$ as
\[
g(i,u) = \sum_jp_{ij}(u)\tilde{g}(i,u,j)
\]
\item
The total cost associated with initial state $i$ and policy $\pi=\{\mu_0,\mu_1,\dots\}$ is given by:
\[
J_{\pi}(i) = \lim_{N\to\infty}\mathbb{E}\left\{
\sum_{k=0}^{N-1}\alpha^kg(x_k,\mu_k(x_k))\middle|x_0=i
\right\}
\]
where $\alpha\in(0,1]$.
We will pose some assumptions to guarantee the existence of above limit.
\item
For special policies $\pi=\{\mu,\mu,\dots\}$, we use $J_{\mu}(i)$ in place of $J_{\pi}(i)$.

We say that $\mu$ is optimal if
\[
J_{\mu}(i)=J^*(i) = \min_{\pi}J_{\pi}(i)
\]
\end{enumerate}




















\subsection{Stochastic Shortest Path Problems}
\paragraph{Assumptions}
\begin{itemize}
\item
Finite-state system: states $1,\dots,n$ and a special cost-free termination state $t$
\item
There is no discount, i.e., $\alpha=1$
\item
Termination inevitable assumption:
There exists integer $m$ such that for every policy and initial state, there is positive probability that the termination state will be reached after no more than $m$ staages, i.e., for all $\pi$, we have
\[
\rho_{\pi} = \max_{i=1,\dots,n}P\{x_m\ne t\mid x_0=i,\pi\}<1
\]
\end{itemize}
\begin{lemma}
The cost-to-go function $J_{\pi}(i)$ is bounded by
\[
|J_{\pi}(i)|\le\frac{m}{1-\rho}\max_{i=1,\dots,n,u\in U(i)}|g(i,u)|
\]
where $\rho:=\max_{\pi}\rho_{\pi}<1$.
\end{lemma}
\begin{proof}
Note that $\rho_{\pi}<1$, since it depends only on the first $m$ components of $\pi$, and the number of $m$-stage policies is finite.
We may regard $\rho_{\pi}$ as an upper bound on the termination probability over $m$ steps.

Therefore, for any $\pi$ and any initial stae $i$,
\begin{align*}
P\{x_{2m}\ne t\mid x_0=i,\pi\}&=P\{x_{2m}\ne t\mid x_m\ne t,x_0=i,\pi\}\\
&\times P\{x_m\ne t\mid x_0=i,\pi\}\le\rho^2
\end{align*}
and similarly,
\[
P\{x_{km}\ne t\mid x_0=i,\pi\}\le \rho^k,\quad i=1,\dots,n
\]

Therefore, the expected cost~(why expected?) between stage $km$ and stage $(k+1)m-1$ is bounded by
\[
m\rho^k\max_{i=1,\dots,n,u\in U(i)}|g(i,u)|
\]
and further 
\[
|J_\pi(i)|\le \sum_{k=0}^\infty m\rho^k\max_{i=1,\dots,n,u\in U(i)}|g(i,u)| = \frac{m}{1-\rho}\max_{i=1,\dots,n,u\in U(i)}|g(i,u)|
\]


\end{proof}


\begin{proposition}\label{pro:6:1}
The following results hold for the stochastic shortest path problems:
\begin{enumerate}
\item
Given the initial condition $J_0(1),\dots,J_0(n)$, the sequence $J_k(i)$ generated by the \emph{value iteration}
\begin{equation}\label{Eq:6:1}
J_{k+1}(i)=\min_{u\in U(i)}\left[
g(i,u)+\sum_{j=1}^np_{ij}(u)J_k(j)
\right]
\end{equation}
converges to the optimal cost $J^*(i)$ for each $i$. Here $J_k(i)$ denotes the optimal cost starting from state $i$ of a $k$-stage problem, with cost per stage given by $g$, and terminal cost at the end of $k$ stages, given by $J_0$
\item
Bellman equation admits the unique solution, which is the optimal cost-to-go function:
\[
J^*(i) = \min_{u\in U(i)}\left[
g(i,u)+\sum_{j=1}^np_{ij}(u)J^*(j)
\right]
\]
\item
Policy iteration: Fix any stationary policy $\mu$, the costs $J_\mu(1),\dots,J_\mu(n)$ are the unique solution of the equation 
\[
J_\mu(i) = g(i,\mu(i))+\sum_{j=1}^np_{ij}(\mu(i))J_\mu(i),\quad
i=1,\dots,n
\]
Furthermore, given any initial conditions $J_0(1),\dots,J_0(n)$, the sequence $J_k(i)$ generated by the DP iteration
\[
J_{k+1}(i) = g(i,\mu(u))+\sum_{j=1}^np_{ij}(\mu(i))J_k(j),\quad
i=1,\dots,n
\]
converges to the cost $J_\mu(i)$ for each $i$
\item
A stationary policy $\mu$ is optimal iff for each state $i$, $\mu(i)$ attains the minimum in the Bellman's equation.
\end{enumerate}
\end{proposition}
\begin{proof}
The idea is to bound the tail of cost-to-go function
\[
\sum_{k=mK}^\infty\mathbb{E}g(x_k,\mu_k(x_k))
\]
\begin{enumerate}
\item
For each positive integer $K$, initial state $x_0$ and policy $\{\mu_0,\mu_1,\dots\}$, we write
\begin{align*}
J_{\pi}(x_0)&=\lim_{N\to\infty}\mathbb{E}\Bigg\{\sum_{k=0}^{N-1}g(x_k,\mu_k(x_k))\Bigg\}\\
&=\mathbb{E}\left\{\sum_{k=0}^{mK-1}g(x_k,\mu_k(x_k))\right\}\\
&+\lim_{N\to\infty}\mathbb{E}\left\{
\sum_{k=mK}^{N-1}g(x_k,\mu_k(x_k))
\right\}
\end{align*}
The expected cost during the $K$th $m$-stage cycle is bounded by $M\rho^k$, where $M=m\max_{i=1,\dots,n, u\in U(i)}|g(i,u)|$, and therefore
\[
\left|
\lim_{N\to\infty}\mathbb{E}\left\{
\sum_{k=mK}^{N-1}g(x_k,\mu_k(x_k))
\right\}
\right|\le M\sum_{k=K}^\infty\rho^k=\frac{\rho^kM}{1-\rho}
\]
Denote $J_0$ as the terminal cost function and $J_0(t)=0$, and we imply
\begin{align*}
\mathbb{E}\{J_0(x_{mK})\}&=\left|\sum_{i=1}^nP(x_{mK}=i\mid x_0,\pi)J_0(i)\right|\\
&\le\left(\sum_{i=1}^nP(x_{mK}=i\mid x_0,\pi)\right)\max_{i=1:n}|J_0(i)|\\
&\le\rho^K\max_{i=1:n}|J_0(i)|
\end{align*}
since the probability that $x_{mK}\ne t$ is bounded by $\rho^K$ for any policy.
Then we bound
\begin{align*}
&-\rho^K\max_{i=1:n}|J_0(i)|+J_{\pi}(x_0)-\frac{\rho^KM}{1-\rho}\\
&\le\mathbb{E}\{J_0(x_{mK})+\sum_{k=0}^{mK-1}g(x_k,\mu_k(x_k))\}\\
&\le\rho^K\max_{i=1:n}|J_0(i)| + J_{\pi}(x_0)+\frac{\rho^KM}{1-\rho}
\end{align*}
The inequality above holds for any policy $\mu$. The minimization over $\mu$ gives
\begin{align*}
&-\rho^K\max_{i=1:n}|J_0(i)|+J^*(x_0)-\frac{\rho^KM}{1-\rho}\\
&\le J_{mK}(x_0)\\
&\le\rho^K\max_{i=1:n}|J_0(i)| + J^*(x_0)+\frac{\rho^KM}{1-\rho}
\end{align*}
Taking $K\to\infty$ gives $\lim_{K\to\infty}J_{mK}(x_0) = J^*(x_0)$.
\item
It suffices to show the uniqueness. Suppose that $J(1),\dots,J(n)$ satisfies the Bellman equation, then the value iteration (\ref{Eq:6:1}) starting from the $J(1),\dots,J(n)$ gives convergence of $J(1),\dots,J(n)$, i.e., optimal cost-to-go.
\item
Given stationary policy $\mu$, consider the modified SSP, i.e., the control constraint is modified as $\tilde{U}(i)=\{\mu(i)\}$, then $J_\mu(1),\dots,J_\mu(n)$ uniquely solves the Bellman's equation for this modified problem, and further more from~(a) the global convergence result is also shown.
\item
The policy $\mu(i)$ is optimal iff
\begin{align*}
J^*(i)&=\min_{u\in U(i)}\left[
g(i,u)+\sum_{j=1}^np_{ij}(u)J^*(j)
\right]\\
&=g(i,\mu(i))+\sum_{j=1}^np_{ij}(\mu(i))J^*(j),\quad
i=1,\dots,n
\end{align*}
By part~(3), $J_\mu(i)=J^*(i)$ for any $i$.
Conversely, if $J_\mu(i)=J^*(i)$ for any $i$, then (2) and (3) imply the equation above.

\end{enumerate}
\end{proof}

\begin{example}
Consider the case where $g(i,u)=1,i=1,\dots,n, u\in U(i)$. It corresponds to a problem where the objective is to terminate as fast as possible on average, where the optimal cost-to-go function $J^*(i)$ is the minimum expected time to terminate starting from $i$. The $J^*(i)$ solves the Bellman equation
\[
J^*(i) = \min_{u\in U(i)}\left[
1+\sum_{j=1}^np_{ij}(u)J^*(j)
\right],\ i=1,\dots,n
\]
If there is only one control at each state, $J^*(i)$ represents the mean first passage time from $i$ to $t$, and these times, denoted as $m_i$, satisfies
\[
m_i = 1+\sum_{j=1}^np_{ij}m_j,\ i=1,\dots,n
\]

\end{example}
\begin{example}[Spider and Fly]
Let the state denotes the distance between spider and fly. For $i\ge2$, we have
\[
p_{ii}=p,\quad
p_{i,(i-1)}=1-2p,\quad
p_{i,(i-2)}=p,\ i\ge2,
\]
and
\[
\begin{array}{lll}
p_{1,1}(M)=2p,&p_{1,0}(M)=1-2p\\
p_{1,2}(\bar{M})=p,&p_{1,1}(\bar{M})=1-2p,&p_{1,0}(\bar{M})=p.
\end{array}
\]
For state $i\ge2$, the Bellman equation is
\begin{subequations}
\begin{equation}
J^*(i) = 1+pJ^*(i)+(1-2p)J^*(i-1)+pJ^*(i-2),\quad i\ge2
\end{equation}
with $J^*(0)=0$.
For the $i=1$, the Bellman equation is
\begin{equation}
J^*(1)=1+\min\{2pJ^*(1),pJ^*(2)+(1-2p)J^*(1)\}
\end{equation}
Firstly we solve $J^*(2)$ in terms of $J^*(1)$:
\[
J^*(2)=\frac{1}{1-p}+\frac{(1-2p)J^*(1)}{1-p}
\]
and substituting $J^*(2)$ into $J^*(1)$ gives
\[
J^*(1) = 1+\min\left[2pJ^*(1),\frac{p}{1-p}+\frac{(1-2p)J^*(1)}{1-p}\right]
\]
Solving this equation suffices to solve cases
\[
\left\{
\begin{aligned}
J^*(1)&=1+2pJ^*(1)\\
2pJ^*(1)&\le \frac{p}{1-p}+\frac{(1-2p)J^*(1)}{1-p}
\end{aligned}
\right.
\]
and
\[
\left\{
\begin{aligned}
J^*(1)&=1+\frac{p}{1-p}+\frac{(1-2p)J^*(1)}{1-p}\\
2pJ^*(1)&\ge \frac{p}{1-p}+\frac{(1-2p)J^*(1)}{1-p}
\end{aligned}
\right.
\]
and therefore
\[J^*(1)=
\left\{
\begin{aligned}
1/(1-2p)&\quad\text{if $p\le 1/3$}\\
1/p&\quad\text{if $p\ge1/3$}
\end{aligned}
\right.
\]
\end{subequations}
\end{example}

\paragraph{Error Bound for Value Iteration}
From previous proof we can see that $|J_{mK}(i) - J^*(i)|$ is bounded by a constant multiple of $\rho^k$. We can strengthen this bound, i.e., for all $k$ and $j$,
\[
J_{k+1}(j) +(N^*(j) - 1)c_{k,l}\le J^*(j)\le J_{\mu^K}(j)\le J_{k+1}(k)+(N^*(j) - 1)c_{k,u}
\]
where $\mu^k$ is the policy derived in $k$-th iteration.
Moreover,
\begin{itemize}
\item
$N^*(j)$ denotes the average number of stages to reach $t$, starting from $j$ and using some optimal stationary policy
\item
$N^k(j)$ denotes the average number of stages to reach $t$, starting from $j$ and using the stationary policy $\mu^k$
\item
$c_{k,l}=\min_{i=1:n}J_{k+1}(i) - J_k(i)$ and $c_{k,u}=\max_{i=1:n}J_{k+1}(i) - J_k(i)$
\end{itemize}=
\paragraph{Policy Iteration}
Start with a stationary policy $\mu^0$, and generate a sequence of new policies $\mu^1,\mu^2,\dots$
\begin{subequations}
\begin{enumerate}
\item
Given the policy $\mu^k$, perform a policy evaluation step, which computes $J_{\mu^k}(i)$ for $i=1:n$, as the solution to the system
\begin{equation}\label{Eq:6:3:a}
J(i) = g(i,\mu^k(i))+\sum_{j=1}^np_{ij}(\mu^k(i))J(j),\ i=1,\dots,n
\end{equation}
\item
Then perform a policy improvement step, which computes the new policy $\mu^{k+1}$ as
\begin{equation}\label{Eq:6:3:b}
\mu^{k+1}(i) = \arg\min_{u\in U(i)}\left[
g(i,u)+\sum_{j=1}^np_{ij}(u)J_{\mu^k}(j)
\right]
\end{equation}
\item
This process is repeated until $J_{\mu^{k+1}}(i)=J_{\mu^k}(i)$ for all $i$, and terminates with the policy $\mu^k$.
\end{enumerate}
\end{subequations}
\begin{proposition}
The policy iteration algorithm generates an improving sequence of policies, and terminates with an optimal policy.
\end{proposition}
\begin{proof}
For any $k$, consider the value-iteration-like sequence generated by the recursion
\[
J_{N+1}(i) = g(i,\mu^{k+1}(i))+\sum_{j=1}^np_{ij}(\mu^{k+1}(i))J_N(j),\ i=1:n,\ N\ge0,
\]
and $J_0(i) = J_{\mu^k}(i)$ for $i=1:n$.
By (\ref{Eq:6:3:a}) and (\ref{Eq:6:3:b}), 
\begin{align*}
J_0(i)&=g(i,\mu^k(i))+\sum_{j=1}^np_{ij}(\mu^k(i))J_0(j)\\
&\ge g(i,\mu^{k+1}(i))+\sum_{j=1}^np_{ij}(\mu^{k+1}(i))J_0(j)\\
&=J_1(i)\\
&\ge g(i,\mu^{k+1}(i))+\sum_{j=1}^np_{ij}(\mu^{k+1}(i))J_1(j)=J_2(i)\\
&\ge J_3(i)\ge\cdots\ge
\end{align*}
Since $J_N(i)\to J_{\mu^{k+1}}(i)$, we imply $J_0(i)\ge J_{\mu^{k+1}}(i)$. Therefore, the sequence of generated policies is improving, and since the number of stationary policies is finite, we will obtain a finite of iterations to obtain $J_{\mu^k}(i) = J_{\mu^{k+1}}(i)$.

Therefore, after finite iterations we obtain
\[
J_{\mu^k}(i) = \min_{u\in U(i)}\left[
g(i,u)+\sum_{j=1}^np_{ij}(u)J_{\mu^k}(j)
\right],\ i=1:n.
\]

\end{proof}

\paragraph{Linear Programming}
Suppose we use value iteration to generate $J_k=(J_k(1),\dots,J_k(n))$, starting with an initial condition vector $J_0=(J_0(1),\dots,J_0(n))$ s.t.
\[
J_0(i)\le\min_{u\in U(i)}\left[g(i,u)+\sum_{j=1}^np_{ij}(u)J_0(j)\right]
\]
Then we will have $J_k(i)\le J_{k+1}(i)$ for all $k$ and $i$. By proposition~(\ref{pro:6:1}) we imply $J_0(i)\le J^*(i)$. Therefore, $J^*$ is the largest $J$ such that
\[
J(i)\le g(i,u)+\sum_{j=1}^np_{ij}(u)J(j),\ i=1:n
\]
In particular, $J^*(1),\dots,J^*(n)$ solves the LP of maximzing $\sum_iJ(i)$ w.r.t. the constraint above.



\section{Discounted Problems}
\begin{proposition}
\begin{subequations}
The following holds for the discounted problem:
\begin{enumerate}
\item
The value iteration algorithm
\begin{equation}
J_{k+1}(i) = \min_{u\in U(i)}\left[g(i,u)+\alpha\sum_{j=1}^np_{ij}(u)J_k(j)\right],\ i=1:n
\end{equation}
converges to the optimal cost $J^*(i)$ for $i=1:n$, starting from any initial conditions $J_0(1),\dots,J_0(n)$
\item
The optimal costs $J^*(1),\dots,J^*(n)$ of the discounted problem satisfies the Bellman equation
\begin{equation}
J^*(i) = \min_{u\in U(i)}\left[
g(i,u)+\alpha\sum_{j=1}^np_{ij}(u)J^*(j)
\right]
\end{equation}
and in fact they are the unique solution of this equation.
\item
For any stationary policy $\mu$, the costs $J_\mu(1:n)$ are the unique solution to the syste,
\[
J_\mu(i) = g(i,\mu(i))+\alpha\sum_{j=1}^np_{ij}(\mu(i))J_\mu(j), \ i=1:n
\]
Furthermore, given any initial conditions $J_0(1),\dots,J_0(n)$, the sequence $J_k(i)$ generated by the DP iteration
\[
J_{k+1}(i)=g(i,\mu(i))+\alpha\sum_{j=1}^np_{ij}(\mu(i))J_k(j),\ i=1:n
\]
converges to the cost $J_\mu(i)$ for each $i$
\item
A stationary policy $\mu$ is optimal iff for each state $i$, the $\mu(i)$ attains the minimum in Bellman's equation.
\item
The policy iteration algorithm
\[
\mu^{k+1}(i)=\arg\min_{u\in U(i)}\left[g(i,u)+\alpha\sum_{j=1}^np_{ij}(u)J_{\mu^k}(j)\right],\ i=1:n
\]
generates an improving sequence of policies and terminates with an optimal policy
\end{enumerate}
\end{subequations}
\end{proposition}

\begin{example}
The optimal value function $J^*$ is the unique solution to the Bellman's equation
\[
J^*(x)=\max\left[x,\frac{\mathbb{E}J^*(\omega)}{1+r}\right]
\]
The optimal reward is characterized by the benchmark
\[
\bar{\alpha}=\frac{\mathbb{E}J^*(\omega)}{1+r}
\]

\end{example}

\begin{example}[Manufacturering]
Bellman equation: for $i=0:n-1$,
\[
J^*(i) = \min[K+\alpha(1-p)J^*(0)+\alpha pJ^*(1),ci+\alpha(1-p)J^*(i)+\alpha pJ^*(i+1)]
\]
and
\[
J^*(n) = K+\alpha(1-p)J^*(0)+\alpha pJ^*(1)
\]
We can show that $J^*(i)$ is monotonicially increasing in $i$, and we can argue that if processing a batch of $m$ orders is optimal, then processing a batch of $m+1$ orders is also optimal.
\end{example}




\section{Average cost per stage problems}
Consider the optimization with the average cost per stage starting from a state $i$:
\[
J_{\pi}(i) = \lim_{N\to\infty}\frac{1}{N}\mathbb{E}\left\{
\sum_{k=0}^{N-1}g(x_k,\mu_k(x_k))\middle|x_0=i
\right\}
\]
For most problems of interest the average cost per stage of a policy and the optimal average cost per stage are independent of the initial state.
\paragraph{Assumption}
one of the states, say $n$ is such that for some integer $m>0$, and all initial states and all policies, $n$ is visited with positive propability at least once within the first $m$ stages.

We coonect the average cost problem with SSP. Modify the transition probability by introducing one slack node. Fix the expected cost per stage cost incurred at stage $i$ to be
\[
g(i,u)-\lambda^*
\]
where $\lambda^*$ is the optimal average cost per stage starting from the special state $n$

\begin{proposition}
The following hold for the average cost per stage problem
\begin{enumerate}
\item
The optimal average cost $\lambda^*$ is the same for all initial states together with some vector $h^*=\{h^*(1),\dots,h^*(n)\}$ satisfying the Bellman's equation
\begin{equation}
\lambda^*+h^*(i) = \min_{u\in U(i)}\left[g(i,u)+\sum_{j=1}^np_{ij}(u)h^*(j)\right],\ i=1:n
\end{equation}
Furthermore, if $\mu(i)$ attains the minimum in the above equation for all $i$, the stationary policy $\mu$ is optimal. For all vectors $h^*$ satisfying the equation above, there exists a unique vector for which $h^*(n)=0$.
\item
If a scalar $\lambda$ and a vector $h=\{h(1),\dots,h(n)\}$ satisfy the Bellman equation, then $\lambda$ is the average optimal cost per stage for each initial state.
\item
Given a stationary policy $\mu$ with corresponding average cost per stage $\lambda_{\mu}$, then tehre is a unique vectors $h_{\mu}=\{h_\mu(1),\dots,h_\mu(n)\}$ such that $h_\mu(n)=0$, and
\[
\lambda_\mu+h_\mu(i)=g(i,\mu(i))+\sum_{j=1}^np_{ij}(\mu(i))h_\mu(j),\ i=1:n
\]

\end{enumerate}
\end{proposition}

\begin{example}
The Bellman's equation takes the form
\[
\lambda^*+h^*(i)=\min[K+(1-p)h^*(0)+ph^*(1),ci+(1-p)h^*(i)+ph^*(i+1)],\ i=0:n-1
\]
\[
\lambda^*+h^*(n) = K+(1-p)h^*(0)+ph^*(1)
\]
\end{example}


\paragraph{Value Iteration}
\[
J_{k+1}(i)=\min_{u\in U(i)}\left[g(i,u)+\sum_{j=1}^np_{ij}(u)J_k(j)\right],\ i=1,\dots,n
\]
Then $\lim_{k\to\infty}\frac{J_k(i)}{k}=\lambda^*$









\subsection{Value Iteration}
$\alpha=1$, given $J_0(x)$:
\[
J_{k+1}(x) = \min_{u\in U}\mathbb{E}_\omega
\left\{
g(x,u,\omega)+J_k(f(x,u,\omega))
\right\}
\]
Reverse the index for infinite horizon case.

\subsection{Bellman Equation}
\[
J^*(x) = \min_{u\in U}\mathbb{E}_\omega[g(x,u,\omega)+J^*(x,u,\omega)]
\]
wher $x\in\{1,2,3\dots,n\}$.
The solution will be
\[
J^*(x) = \begin{bmatrix}
J^*(1)&\cdots&J^*(n)
\end{bmatrix}
\]
This will be the \emph{unique} solution to the bellman equation.

\subsection{Policy Iteration}



\subsection{Stochastic Shortest Path}
\begin{enumerate}
\item
Special Termination State $t$:
\[
P_{t,t}(u)=1,\ \forall u
\]
\item
Cost-free termination cost: $g(t,u)=0,\forall u$
\item
Assumption of inevitable termination:
We have the ``cost-free'' termination state, i.e., $g(t,u)=0, P_{t,t}(u)=1,\forall u$.

There exists some integer $M$ from any initial state $x_0=i$

For any admissible policy $\pi$,
\[
\rho_{\pi}:=\max_{i}P(x_M\ne t\mid x_0=i,\pi)<1
\]
\end{enumerate}

For ang given $\pi =(\mu_0,\mu_1,\dots)$, study $J_{\pi}(x_0)$. Bounded or Not?
\[
J_{\pi}(x_0)
=\lim_{N\to\infty}
\mathbb{E}
\left\{\alpha^k
\sum_{k=0}^Ng(x_k,\mu_k(x_k))
\right\},\quad
\alpha = 1
\]
where $g(x_k,\mu_k(x_k)) = \sum_{j=1}^ng(x_k,\mu_k(x_k),j)P_{x_i,j}(\mu_k(x_k))$
\[
J_{k+1}(i)
=
\min_{u\in U(i)}
\left\{
g(i,u)+\sum_{j=1}^nP_{i,j}(u)J_k(j)
\right\}
\]
Start with any initial function $J_0(i), i=1,\dots,n$, and $J_0(t)=0$.
As $k\to\infty$, $\lim_{k\to\infty}J_k(x_0) = J^*(x_0)$,
which is called the vanishing total cost.

\begin{definition}
Let $J^*(x_0)$ be the best cost-to-go function among all the admissiable policy.
\begin{align*}
J^*(x_0)&=\min_{\pi}J_{\pi}(x_0)
:=
\min_{\mu_0,\dots}\mathbb{E}\left(
\sum_{\ell=0}^\infty g(x_\ell,\mu(x_{\ell}))
\right)
\\
J_k(x_0)&=\min_{\mu_0,\dots,\mu_{k-1}}
\mathbb{E}
\left\{
\sum_{l=0}^{k-1}g(x_k,\mu_k(x_k))+J_0(x_k)
\right\}
\end{align*}
\end{definition}

\begin{enumerate}
\item
Give $J_0(i)$ for $i=1,\dots,n$
\item
\[
J_{k+i}(i) = \min_{u\in U(i)}\left\{
g(x,u)+\sum_{j=1}^nP_{ij}(u)+J_k(j)
\right\}
\]
\item
As $k\to\infty$, converges to $J^*(i)$
\end{enumerate}

Measure the distance between $J_k(\cdot)$ and $J_{k+1}(\cdot)$:
\[
\|J_k - J_{k+1}\|_{1,2,\infty}
\]

\subsection{Bellman Equation}
\[
J^*(i) = \min_{u\in U(i)}
\left\{
g(i,u)
+
\sum_{j=1}^nP_{ij}(u)J^*(j)
\right\}
\]
As long as $J^*(\cdot)$ satisfies the $n$ equations above, we insist that $J^*$ is really the optimal solution.
Moreover, the solution to the equation above is unique.

\[
J_{\mu}(i) = g(i,\mu(i)) + \sum_{j=1}^nP_{ij}(\mu(i))J_{\mu}(j), \ i=1,\dots,n
\]
Aim at the vector $[J_{\mu}(1),\dots,J_{\mu}(n)]$

Just keep implement the operator
\[
J_{k+1}(i) = g(i,\mu(i))+\sum_{j=1}^nP_{i,j}(\mu(i))J_k(j)
\]
until $J_{k+1}(i)$ converges.

How to compute the stationary policy:
\begin{proposition}[Necessary and Sufficient Optimality Condition]
The policy $\mu$ is optimal if and only if $\mu$ satisfies the Bellman equation.
\end{proposition}



\subsection{Discounted Problems}
\begin{itemize}
\item
State:\{1,\dots,n\}
\end{itemize}
Introduce the artificial $t$, and introduce the transition from any state to $t$.


\paragraph{Value Iteration}
Start with any $J_0(1),\dots,J_0(n)$.
Construct
\[
J_{k+1} = \min_{u\in U}\left\{
g(i,u)+\alpha\cdot\sum_{j=1}^np_{i,j}(u)J_k(j)
\right\}
\]
and therefore $J^*(i) = \lim_{k\to\infty}J_k(i)$

Bellman equation:
find $J^*(1),\dots,J^*(n)$ such that
\[
J^*(i) = \min_{u\in U}\left\{
g(i,u)+\alpha\cdot\sum_{j=1}^np_{i,j}(u)J^*(j)
\right\},\ i=1,\dots,n
\]
\paragraph{
System performance of stationary policy $\mu$
}
Solve
\[
J_{\mu}(i) = g(i,\mu(i))+\alpha\sum_{j=1}^np_{ij}(\mu(i))J_{\mu}(j),\ i=1,\dots,n
\]
Then start with any $J_0(1),\dots,J_0(n)$, we have
\[
J_{k+1}(i) = g(i,\mu(i))+\alpha\sum_{j=1}^np_{i,j}(\mu(i))
J_k(j),\ \forall i
\]
then $J_{\mu}(i) = \lim_{k\to\infty}J_k(i)$.

\paragraph{Policy Iteration}
Once we get $J_{\mu_k}(i)$ for $i=1,\dots,n$, improve
\[
\mu_{k+1}(i) = \arg\min_{u\in U(i)}
\left\{
g(i,u)+\alpha\sum_{j=1}^np_{i,j}(u)J_{\mu_k}(i)
\right\}
\]


\begin{example}[Asset Selling]




\end{example}
