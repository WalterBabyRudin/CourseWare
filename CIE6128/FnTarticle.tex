% !Mode:: "TeX:DE:UTF-8:Main"
%
%
%JOURNAL CODE  SEE DOCUMENTATION

\input{parameter.tex}

\begin{document}
\makeabstracttitle
\begin{abstract}
How does deep learning work? Without any guidance, it is very difficult to find the correct setting that makes a deep neural network perform well. Researchers have spent a few decades to find the right combination of choices that lead to recent success of deep neural-nets. However, most, if not all, existing courses and textbooks did not discuss the underlying theory in depth. In this course, we will go through the current theoretical understanding of neural networks based on research in recent years. Topics include the theory of training dynamics (smart initialization and dynamical isometry, normalization methods), popular algorithms (SGDR, Adam, etc.), landscape of non-convex optimization, adversarial attacks and robustness, GANs (generative adversarial networks), etc.)
\end{abstract}


\input First_lecture/First.tex

\input Second_lecture/First.tex

\input Third_lecture/First.tex

\input Forth_lecture/First.tex

\input Fifth_lecture/First.tex

\input Sixth_lecture/First.tex

\input Seventh_lecture/First.tex

\input Eigth_lecture/First.tex

\input Nineth_lecture/First.tex
%BACKMATTER SEE DOCUMENTATION
\backmatter  % references, restarts sample

\appendix
\input Appendix/First.tex

\printbibliography

\end{document}
