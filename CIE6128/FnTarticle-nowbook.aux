\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\BKM@entry[2]{}
\abx@aux@sortscheme{nyt}
\abx@aux@refcontext{nyt/global/}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Editor-in-Chief}{v}{section*.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Associate Editors}{v}{section*.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\thispagestyle {empty}}
\BKM@entry{id=1,dest={636861707465722E31},srcline={1}}{496E74726F64756374696F6E20746F2044656570204C6561726E696E67}
\BKM@entry{id=2,dest={73656374696F6E2E312E31},srcline={3}}{4D6F7469766174696F6E}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to Deep Learning}{3}{chapter.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{3}{section.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Example: is AI like Alchemy?}{3}{paragraph*.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Numerical Results for solving $\qopname  \relax m{min}_{W_1,W_2}\mathaccentV {hat}05E{\mathbb  {E}}\delimiter "026B30D W_1W_2x-Ax\delimiter "026B30D ^2$\relax }}{3}{figure.caption.7}}
\BKM@entry{id=3,dest={73656374696F6E2E312E32},srcline={41}}{4F75746C696E65}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Comment from Ruoyu Sun}{4}{paragraph*.8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.2}Outline}{4}{section.1.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Pre-requisite}{4}{paragraph*.9}}
\BKM@entry{id=4,dest={73656374696F6E2E312E33},srcline={66}}{4E657572616C204E6574776F726B204261736973}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Course Objective and Audience}{5}{paragraph*.10}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.3}Neural Network Basis}{5}{section.1.3}}
\newlabel{sec:1:3}{{1.3}{5}{Neural Network Basis}{section.1.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Example of a $3$-layer fully-connected neural network.\relax }}{6}{figure.caption.11}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1:2}{{1.2}{6}{Example of a $3$-layer fully-connected neural network.\relax }{figure.caption.11}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Why $\&$ When $\&$ How do we need neural-nets?}{6}{paragraph*.12}}
\BKM@entry{id=5,dest={73656374696F6E2E312E34},srcline={139}}{4772616469656E74204578706C6F73696F6E2F56616E697368696E67}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.4}Gradient Explosion/Vanishing}{7}{section.1.4}}
\newlabel{sec:1:4}{{1.4}{7}{Gradient Explosion/Vanishing}{section.1.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Solving by Classical Gradient Descent}{8}{paragraph*.13}}
\BKM@entry{id=6,dest={636861707465722E32},srcline={1}}{4261636B2050726F7061676174696F6E20616E6420496E697469616C697A6174696F6E}
\BKM@entry{id=7,dest={73656374696F6E2E322E31},srcline={2}}{526576696577}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {2}Back Propagation and Initialization}{9}{chapter.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Review}{9}{section.2.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Function Shape of $(w^7-1)^2$\relax }}{9}{figure.caption.14}}
\newlabel{Fig:2:1}{{2.1}{9}{Function Shape of $(w^7-1)^2$\relax }{figure.caption.14}{}}
\BKM@entry{id=8,dest={73656374696F6E2E322E32},srcline={33}}{4261636B2050726F7061676174696F6E}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{How to rescue the gradient vanishing/explosion during DL training?}{10}{paragraph*.15}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Back Propagation}{10}{section.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Understanding BP in Level I: Scalar Form of Gradient}{10}{paragraph*.16}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Step 1: Decompose into multiple paths}{11}{paragraph*.18}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Step 2: Take gradient of each path by Chain rule}{11}{paragraph*.19}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Step 3: Take the sum of results from each path}{12}{paragraph*.20}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Understanding BP in Level II: Matrix Form of Gradient}{12}{paragraph*.21}}
\newlabel{pro:2:1}{{2.1}{13}{Vector-Function Chain Rule}{proposition.2.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Diagram for the operator $F$\relax }}{13}{figure.caption.22}}
\newlabel{Eq:2:1}{{2.1}{15}{}{equation.2.2.1}{}}
\newlabel{lemma:2:1}{{2.1}{15}{}{theorem.2.2.1}{}}
\newlabel{Eq:2:2a}{{2.2a}{15}{}{equation.2.2.1}{}}
\newlabel{Eq:2:2b}{{2.2b}{15}{}{equation.2.2.2}{}}
\newlabel{Eq:2:2c}{{2.2c}{15}{}{equation.2.2.3}{}}
\newlabel{Eq:2:2d}{{2.2d}{15}{}{equation.2.2.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{BP for General Deep Non-linear Network}{16}{paragraph*.23}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Diagram for the operator $F$\relax }}{16}{figure.caption.24}}
\newlabel{Eq:2:3a}{{2.3a}{16}{BP for General Deep Non-linear Network}{equation.2.2.1}{}}
\newlabel{Eq:2:3b}{{2.3b}{16}{BP for General Deep Non-linear Network}{equation.2.2.2}{}}
\newlabel{Eq:2:3d}{{2.3c}{16}{BP for General Deep Non-linear Network}{equation.2.2.3}{}}
\newlabel{Eq:2:3e}{{2.3d}{16}{BP for General Deep Non-linear Network}{equation.2.2.4}{}}
\BKM@entry{id=9,dest={73656374696F6E2E322E33},srcline={296}}{496E697469616C697A6174696F6E206D6574686F647320666F722068616E646C696E6720547261696E696E6720446966666963756C7479}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Initialization methods for handling Training Difficulty}{17}{section.2.3}}
\newlabel{sec:2:3}{{2.3}{17}{Initialization methods for handling Training Difficulty}{section.2.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Supporting Analysis}{18}{paragraph*.25}}
\newlabel{Eq:2:4}{{2.4}{18}{Supporting Analysis}{equation.2.3.4}{}}
\BKM@entry{id=10,dest={636861707465722E33},srcline={1}}{54616D696E67204578706C6F73696F6E2F56616E697368696E673A20496E697469616C697A6174696F6E}
\BKM@entry{id=11,dest={73656374696F6E2E332E31},srcline={3}}{526576696577696E67}
\BKM@entry{id=12,dest={73656374696F6E2E332E32},srcline={26}}{4D6F7469766174696F6E}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {3}Taming Explosion/Vanishing: Initialization}{21}{chapter.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}Reviewing}{21}{section.3.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}Motivation}{21}{section.3.2}}
\newlabel{exp:3:1}{{3.1}{22}{}{example.3.1}{}}
\abx@aux@cite{Glorot10understandingthe}
\abx@aux@segm{0}{0}{Glorot10understandingthe}
\abx@aux@cite{NIPS2018_7338}
\abx@aux@segm{0}{0}{NIPS2018_7338}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Bibliography}{24}{paragraph*.26}}
\BKM@entry{id=13,dest={73656374696F6E2E332E33},srcline={163}}{47656E6572616C2041637469766174696F6E}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.3}General Activation}{25}{section.3.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Problem Setting}{25}{paragraph*.27}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}The scalar-input one-layer case}{25}{subsection.3.3.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}The vector-input one-layer case}{25}{subsection.3.3.2}}
\abx@aux@cite{NIPS2016_6322}
\abx@aux@segm{0}{0}{NIPS2016_6322}
\abx@aux@cite{Bill86}
\abx@aux@segm{0}{0}{Bill86}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}The vector-input multi-layer case}{26}{subsection.3.3.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Mean-field approximation}{26}{paragraph*.28}}
\BKM@entry{id=14,dest={73656374696F6E2E332E34},srcline={297}}{44796E616D6963616C2049736F6D65747279}
\abx@aux@cite{DBLP}
\abx@aux@segm{0}{0}{DBLP}
\abx@aux@cite{Pennington2018TheEO}
\abx@aux@segm{0}{0}{Pennington2018TheEO}
\abx@aux@cite{pmlr-v80-xiao18a}
\abx@aux@segm{0}{0}{pmlr-v80-xiao18a}
\abx@aux@cite{li2018on}
\abx@aux@segm{0}{0}{li2018on}
\abx@aux@cite{08987}
\abx@aux@segm{0}{0}{08987}
\abx@aux@cite{zhang2018residual}
\abx@aux@segm{0}{0}{zhang2018residual}
\newlabel{Eq:3:1}{{3.1}{28}{}{equation.3.3.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.4}Dynamical Isometry}{28}{section.3.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Bibliography}{28}{paragraph*.29}}
\abx@aux@cite{Saxe14exactsolutions}
\abx@aux@segm{0}{0}{Saxe14exactsolutions}
\abx@aux@segm{0}{0}{Saxe14exactsolutions}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Dynamical Isometry for Linear Networks}{29}{subsection.3.4.1}}
\BKM@entry{id=15,dest={636861707465722E34},srcline={1}}{546872656520547269636B7320696E20547261696E696E67206F66204E657572616C204E6574776F726B}
\BKM@entry{id=16,dest={73656374696F6E2E342E31},srcline={3}}{526576696577696E67}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {4}Three Tricks in Training of Neural Network}{31}{chapter.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.1}Reviewing}{31}{section.4.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Motivation}{31}{paragraph*.30}}
\BKM@entry{id=17,dest={73656374696F6E2E342E32},srcline={38}}{496E7469616C697A6174696F6E3A2044796E616D6963616C2049736F6D65747279}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.2}Intialization: Dynamical Isometry}{32}{section.4.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces  The pdf of eigenvalues for the scaled (symmetric) random matrix $\frac  {1}{\sqrt  {N}}W$ with $N=1000$ and $w_{i,j}$ follows normal distribution.{The simulation code is in {{https://www.mathworks.com/matlabcentral/fileexchange/46464-wigner-s-semicircle-law}}} \relax }}{32}{figure.caption.31}}
\abx@aux@segm{0}{0}{DBLP}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{The possibility of dynamical isometry}{33}{paragraph*.32}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Summarization of the results for the possibilities for realizing dynamical isometry for deep neural networks with different types of activations and initializations\relax }}{33}{table.caption.33}}
\newlabel{tab:1}{{4.1}{33}{Summarization of the results for the possibilities for realizing dynamical isometry for deep neural networks with different types of activations and initializations\relax }{table.caption.33}{}}
\abx@aux@segm{0}{0}{pmlr-v80-xiao18a}
\abx@aux@segm{0}{0}{pmlr-v80-xiao18a}
\BKM@entry{id=18,dest={73656374696F6E2E342E33},srcline={114}}{4261746368204E6F726D616C697A6174696F6E}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Why ``Orthogonal'' is ``Difficult''}{34}{paragraph*.34}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.3}Batch Normalization}{34}{section.4.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Motivation}{34}{paragraph*.35}}
\abx@aux@segm{0}{0}{Glorot10understandingthe}
\newlabel{Eq:4:1}{{4.1}{36}{Toy Example}{equation.4.3.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Method 1: Pure Algorithmic Correction}{36}{paragraph*.36}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Method 2: Constrained optimization}{36}{paragraph*.37}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Method 3: New way of Formulation}{36}{paragraph*.38}}
\newlabel{Eq:4:2}{{4.2}{37}{Method 3: New way of Formulation}{equation.4.3.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Motivation for Batch Normalization\nobreakspace  {}(BN)}{37}{paragraph*.39}}
\newlabel{Eq:4:3}{{4.3}{37}{Motivation for Batch Normalization~(BN)}{equation.4.3.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Adding the Bacth Normalization process in the \emph  {second} layer\relax }}{37}{figure.caption.40}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Gradient Computation if adding the BN}{38}{paragraph*.41}}
\newlabel{Eq:4:4}{{4.4}{38}{Representation Power}{equation.4.3.4}{}}
\abx@aux@cite{Wu_2018_ECCV}
\abx@aux@segm{0}{0}{Wu_2018_ECCV}
\BKM@entry{id=19,dest={73656374696F6E2E342E34},srcline={368}}{5265734E6574}
\abx@aux@cite{He2016res}
\abx@aux@segm{0}{0}{He2016res}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.4}ResNet}{40}{section.4.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Motivation}{40}{paragraph*.43}}
\abx@aux@cite{frankle2018the}
\abx@aux@segm{0}{0}{frankle2018the}
\abx@aux@cite{WinNT}
\abx@aux@segm{0}{0}{WinNT}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces  Training error\nobreakspace  {}(left) and test error\nobreakspace  {}(right) on CIFAR-$10$ with $20$-layer and $56$-layer ``plain'' networks. The deeper network has higher training error, and thus test error. Similar phenomena on ImageNet is also observed. \relax }}{41}{figure.caption.44}}
\newlabel{fig:4:3}{{4.3}{41}{Training error~(left) and test error~(right) on CIFAR-$10$ with $20$-layer and $56$-layer ``plain'' networks. The deeper network has higher training error, and thus test error. Similar phenomena on ImageNet is also observed. \relax }{figure.caption.44}{}}
\abx@aux@cite{srivastava2015highway}
\abx@aux@segm{0}{0}{srivastava2015highway}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Solution}{42}{paragraph*.45}}
\BKM@entry{id=20,dest={636861707465722E35},srcline={1}}{5265734E657420496E697469616C697A6174696F6E20616E64204C616E64736361706520416E616C79736973}
\BKM@entry{id=21,dest={73656374696F6E2E352E31},srcline={3}}{526576696577696E67}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {5}ResNet Initialization and Landscape Analysis}{43}{chapter.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.1}Reviewing}{43}{section.5.1}}
\BKM@entry{id=22,dest={73656374696F6E2E352E32},srcline={50}}{496E697469616C697A6174696F6E20666F72205265734E6574}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.2}Initialization for ResNet}{44}{section.5.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces ResNet framework with Relu activation\relax }}{44}{figure.caption.46}}
\abx@aux@segm{0}{0}{He2016res}
\abx@aux@cite{Zhang2000}
\abx@aux@segm{0}{0}{Zhang2000}
\abx@aux@cite{Balduzzi2017}
\abx@aux@segm{0}{0}{Balduzzi2017}
\abx@aux@segm{0}{0}{Balduzzi2017}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Bibilogrphy}{46}{paragraph*.47}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Scaling of the Residuals}{46}{paragraph*.48}}
\abx@aux@cite{Szegedy2016Inceptionv4IA}
\abx@aux@segm{0}{0}{Szegedy2016Inceptionv4IA}
\abx@aux@segm{0}{0}{zhang2018residual}
\abx@aux@segm{0}{0}{Glorot10understandingthe}
\abx@aux@cite{glorot2011relu}
\abx@aux@segm{0}{0}{glorot2011relu}
\abx@aux@cite{DDR2919332}
\abx@aux@segm{0}{0}{DDR2919332}
\abx@aux@segm{0}{0}{He2016res}
\BKM@entry{id=23,dest={73656374696F6E2E352E33},srcline={185}}{4C616E647363617065206F66204E657572616C2D4E657473}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Is normalization fundamental?}{47}{paragraph*.49}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.3}Landscape of Neural-Nets}{48}{section.5.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Motivation}{48}{paragraph*.50}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Positive Result: Linear Network has nice landscape}{48}{subsection.5.3.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Scalar Case Analysis}{48}{paragraph*.51}}
\abx@aux@cite{pmlr-v49-lee16}
\abx@aux@segm{0}{0}{pmlr-v49-lee16}
\abx@aux@cite{ISIT2019}
\abx@aux@segm{0}{0}{ISIT2019}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces $3$-D plot for the loss function $F(u,v) = (1-uv)^2$\relax }}{49}{figure.caption.52}}
\newlabel{fig:5:2}{{5.2}{49}{$3$-D plot for the loss function $F(u,v) = (1-uv)^2$\relax }{figure.caption.52}{}}
\abx@aux@cite{BALDI198953}
\abx@aux@segm{0}{0}{BALDI198953}
\abx@aux@cite{NIPS2016_6112}
\abx@aux@segm{0}{0}{NIPS2016_6112}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Re-thinking Convexity}{50}{paragraph*.53}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Bibliography}{51}{paragraph*.54}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Negative Result: Nonlinearity doesn't necessarily imply Global-optimality for SOSP}{51}{subsection.5.3.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Relu Activation}{51}{paragraph*.55}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces $2$-D plot for the loss function $F(w) = (1-w_+)^2$\relax }}{51}{figure.caption.56}}
\newlabel{fig:5:3}{{5.3}{51}{$2$-D plot for the loss function $F(w) = (1-w_+)^2$\relax }{figure.caption.56}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces $2$-D plot for the loss function $F(w) = (1-w^2)^2$\relax }}{52}{figure.caption.57}}
\newlabel{fig:5:4}{{5.4}{52}{$2$-D plot for the loss function $F(w) = (1-w^2)^2$\relax }{figure.caption.57}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces $2$-D plot for the loss function $F(w) = (1-\text  {sigmoid}(w))^2$\relax }}{52}{figure.caption.58}}
\newlabel{fig:5:5}{{5.5}{52}{$2$-D plot for the loss function $F(w) = (1-\text {sigmoid}(w))^2$\relax }{figure.caption.58}{}}
\BKM@entry{id=24,dest={636861707465722E36},srcline={1}}{4C616E64736361706520416E616C7973697320616E6420526570726573656E746174696F6E}
\BKM@entry{id=25,dest={73656374696F6E2E362E31},srcline={4}}{526576696577696E67}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {6}Landscape Analysis and Representation}{55}{chapter.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.1}Reviewing}{55}{section.6.1}}
\BKM@entry{id=26,dest={73656374696F6E2E362E32},srcline={65}}{204C616E64736361706520616E616C7973697320666F72206E6F6E2D6C696E656172206E657572616C2D6E65747320}
\abx@aux@cite{NIPS19951028}
\abx@aux@segm{0}{0}{NIPS19951028}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Outline}{56}{paragraph*.59}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.2} Landscape analysis for non-linear neural-nets }{56}{section.6.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Negative Result: The sum of two good-landscape function have good landscape }{56}{subsection.6.2.1}}
\abx@aux@segm{0}{0}{NIPS19951028}
\abx@aux@segm{0}{0}{NIPS19951028}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Illustraion of a counter-example for $1$-dimension case\relax }}{57}{figure.caption.60}}
\newlabel{fig:6:1}{{6.1}{57}{Illustraion of a counter-example for $1$-dimension case\relax }{figure.caption.60}{}}
\abx@aux@segm{0}{0}{NIPS19951028}
\BKM@entry{id=27,dest={73656374696F6E2E362E33},srcline={120}}{4F7665722D506172616D65746572697A6564204E6574776F726B73}
\abx@aux@segm{0}{0}{frankle2018the}
\abx@aux@cite{NIPS2015_5784}
\abx@aux@segm{0}{0}{NIPS2015_5784}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.3}Over-Parameterized Networks}{58}{section.6.3}}
\abx@aux@segm{0}{0}{BALDI198953}
\abx@aux@segm{0}{0}{NIPS19951028}
\abx@aux@cite{410380}
\abx@aux@segm{0}{0}{410380}
\abx@aux@cite{ruoyusun2018}
\abx@aux@segm{0}{0}{ruoyusun2018}
\abx@aux@segm{0}{0}{410380}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Does Current Neural-net have too many parameters?}{59}{paragraph*.61}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Bibliography}{59}{paragraph*.62}}
\abx@aux@segm{0}{0}{ruoyusun2018}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces No bad basin\relax }}{60}{figure.caption.63}}
\newlabel{fig:6:2}{{6.2}{60}{No bad basin\relax }{figure.caption.63}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Example of bad basin\relax }}{60}{figure.caption.63}}
\newlabel{fig:6:3}{{6.3}{60}{Example of bad basin\relax }{figure.caption.63}{}}
\abx@aux@cite{43404}
\abx@aux@segm{0}{0}{43404}
\abx@aux@cite{NIPS2018_8095}
\abx@aux@segm{0}{0}{NIPS2018_8095}
\abx@aux@cite{Gotmare2018}
\abx@aux@segm{0}{0}{Gotmare2018}
\abx@aux@cite{DBLPkarol}
\abx@aux@segm{0}{0}{DBLPkarol}
\BKM@entry{id=28,dest={73656374696F6E2E362E34},srcline={216}}{526570726573656E746174696F6E20506F776572}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Empirical Evidence for Landscape}{61}{subsection.6.3.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Bibliography}{61}{paragraph*.64}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.4}Representation Power}{61}{section.6.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Motivation}{61}{paragraph*.65}}
\abx@aux@cite{Hornik1991}
\abx@aux@segm{0}{0}{Hornik1991}
\abx@aux@cite{jiewang2019}
\abx@aux@segm{0}{0}{jiewang2019}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Formulation of Representation Power}{62}{paragraph*.66}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Sufficient Condition for Enough Representation Power}{62}{paragraph*.67}}
\BKM@entry{id=29,dest={636861707465722E37},srcline={1}}{526570726573656E746174696F6E20616E642047414E}
\BKM@entry{id=30,dest={73656374696F6E2E372E31},srcline={3}}{526576696577696E67}
\abx@aux@segm{0}{0}{NIPS19951028}
\abx@aux@segm{0}{0}{ruoyusun2018}
\abx@aux@segm{0}{0}{ruoyusun2018}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {7}Representation and GAN}{63}{chapter.7}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.1}Reviewing}{63}{section.7.1}}
\BKM@entry{id=31,dest={73656374696F6E2E372E32},srcline={31}}{526570726573656E746174696F6E3A2064657074682073657061726174696F6E}
\abx@aux@cite{jiewang20191}
\abx@aux@segm{0}{0}{jiewang20191}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.2}Representation: depth separation}{64}{section.7.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}A simple proof of threhold activation has enough representation power}{64}{subsection.7.2.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces The pulse function can approximate any continuous function.\relax }}{64}{figure.caption.68}}
\newlabel{fig:7:1}{{7.1}{64}{The pulse function can approximate any continuous function.\relax }{figure.caption.68}{}}
\abx@aux@cite{3561150}
\abx@aux@segm{0}{0}{3561150}
\abx@aux@cite{Barron1994}
\abx@aux@segm{0}{0}{Barron1994}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Bibliography}{65}{paragraph*.69}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Depth Separation (Analysis for ReLU Activation)}{66}{subsection.7.2.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces The function $\psi $, which has one ``peak''\relax }}{66}{figure.caption.70}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces The function $\psi ^{(2)}$, which has two ``peaks''\relax }}{66}{figure.caption.71}}
\abx@aux@cite{NIPS2018_7855}
\abx@aux@segm{0}{0}{NIPS2018_7855}
\newlabel{The:7:2}{{7.2}{67}{}{theorem.7.2.2}{}}
\BKM@entry{id=32,dest={73656374696F6E2E372E33},srcline={193}}{47414E}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Summarization}{68}{paragraph*.72}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.3}GAN}{68}{section.7.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Motivation}{69}{paragraph*.73}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Discussion}{69}{paragraph*.74}}
\abx@aux@cite{NIPS2014_5423}
\abx@aux@segm{0}{0}{NIPS2014_5423}
\newlabel{Eq:7:1}{{7.1}{70}{Discussion}{equation.7.3.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Motivation of W-GAN}{71}{paragraph*.75}}
\BKM@entry{id=33,dest={636861707465722E38},srcline={1}}{416476657273617269616C204C6561726E696E67}
\BKM@entry{id=34,dest={73656374696F6E2E382E31},srcline={4}}{496E74726F64756374696F6E20746F20416476657273617269616C204C6561726E696E67}
\abx@aux@cite{43405}
\abx@aux@segm{0}{0}{43405}
\abx@aux@cite{42503}
\abx@aux@segm{0}{0}{42503}
\abx@aux@segm{0}{0}{42503}
\abx@aux@segm{0}{0}{43405}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {8}Adversarial Learning}{73}{chapter.8}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction to Adversarial Learning}{73}{section.8.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Motivation}{73}{paragraph*.76}}
\abx@aux@segm{0}{0}{43405}
\abx@aux@cite{7958570}
\abx@aux@segm{0}{0}{7958570}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Type of attacks}{74}{paragraph*.78}}
\abx@aux@segm{0}{0}{43405}
\abx@aux@segm{0}{0}{42503}
\abx@aux@segm{0}{0}{7958570}
\abx@aux@cite{Chen2017}
\abx@aux@segm{0}{0}{Chen2017}
\abx@aux@cite{Ilyas2018}
\abx@aux@segm{0}{0}{Ilyas2018}
\BKM@entry{id=35,dest={73656374696F6E2E382E32},srcline={72}}{4D617468656D61746963616C20466F726D756C6174696F6E206F66204164766572736172792041747461636B}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Bibliography}{75}{paragraph*.79}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.2}Mathematical Formulation of Adversary Attack}{75}{section.8.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Un-targetted Attack}{75}{subsection.8.2.1}}
\newlabel{Eq:8:1}{{8.1}{75}{Un-targetted Attack}{equation.8.2.1}{}}
\newlabel{Eq:8:1:a}{{8.1a}{75}{Un-targetted Attack}{equation.8.2.1}{}}
\newlabel{Eq:8:1:b}{{8.1b}{75}{Un-targetted Attack}{equation.8.2.2}{}}
\newlabel{Eq:8:1:c}{{8.1c}{75}{Un-targetted Attack}{equation.8.2.3}{}}
\abx@aux@segm{0}{0}{43405}
\newlabel{Eq:8:2:a}{{8.2a}{76}{Un-targetted Attack}{equation.8.2.1}{}}
\newlabel{Eq:8:2:b}{{8.2b}{76}{Un-targetted Attack}{equation.8.2.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Illustration for Fast gradient sign method\relax }}{76}{figure.caption.80}}
\abx@aux@segm{0}{0}{7958570}
\abx@aux@cite{kingma-adam}
\abx@aux@segm{0}{0}{kingma-adam}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Targetted Attack}{77}{subsection.8.2.2}}
\newlabel{Eq:8:3}{{8.3}{77}{Targetted Attack}{equation.8.2.3}{}}
\newlabel{Eq:8:3:a}{{8.3a}{77}{Targetted Attack}{equation.8.2.1}{}}
\newlabel{Eq:8:3:b}{{8.3b}{77}{Targetted Attack}{equation.8.2.2}{}}
\newlabel{Eq:8:3:c}{{8.3c}{77}{Targetted Attack}{equation.8.2.3}{}}
\newlabel{Eq:8:4}{{8.4}{77}{Targetted Attack}{equation.8.2.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Solving Optimization Problems in Adversary Attack}{77}{paragraph*.81}}
\abx@aux@cite{Nesterov2011}
\abx@aux@segm{0}{0}{Nesterov2011}
\BKM@entry{id=36,dest={73656374696F6E2E382E33},srcline={185}}{416476657273617269616C20446566656E7365}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Zeroth-order Optmization}{78}{subsection.8.2.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.3}Adversarial Defense}{78}{section.8.3}}
\abx@aux@cite{SPA3327757}
\abx@aux@segm{0}{0}{SPA3327757}
\BKM@entry{id=37,dest={73656374696F6E2E382E34},srcline={227}}{4F7074696D697A6174696F6E20416C676F726974686D73}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Certified Defense}{79}{subsection.8.3.1}}
\newlabel{Eq:8:5:a}{{8.5a}{79}{Certified Defense}{equation.8.3.1}{}}
\newlabel{Eq:8:5:b}{{8.5b}{79}{Certified Defense}{equation.8.3.2}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces The entries in the table are the robust error, the $\varepsilon $ denotes the norm of perturbation. \relax }}{79}{table.caption.82}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.4}Optimization Algorithms}{80}{section.8.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Part I: Gradient Descent\nobreakspace  {}(GD) method}{80}{subsection.8.4.1}}
\newlabel{Eq:8:6:a}{{8.6a}{81}{Part I: Gradient Descent~(GD) method}{equation.8.4.1}{}}
\newlabel{Eq:8:6:b}{{8.6b}{81}{Part I: Gradient Descent~(GD) method}{equation.8.4.2}{}}
\newlabel{Eq:8:6:c}{{8.6c}{81}{Part I: Gradient Descent~(GD) method}{equation.8.4.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Gradient Descent cannot escape saddle point efficiently\relax }}{82}{figure.caption.83}}
\BKM@entry{id=38,dest={636861707465722E39},srcline={1}}{4F7074696D697A6174696F6E20416C676F726974686D73}
\BKM@entry{id=39,dest={73656374696F6E2E392E31},srcline={3}}{526576696577696E67}
\abx@aux@cite{Razaviyayn2014SuccessiveCA}
\abx@aux@segm{0}{0}{Razaviyayn2014SuccessiveCA}
\BKM@entry{id=40,dest={73656374696F6E2E392E32},srcline={18}}{56617269616E7473206F66204772616469656E742044657363656E74205C2847445C29204D6574686F64}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {9}Optimization Algorithms}{83}{chapter.9}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.1}Reviewing}{83}{section.9.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.2}Variants of Gradient Descent\nobreakspace  {}(GD) Method}{83}{section.9.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Scaled GD}{83}{subsection.9.2.1}}
\newlabel{Eq:9:1}{{9.1}{84}{}{equation.9.2.1}{}}
\newlabel{Eq:9:2}{{9.2}{84}{}{equation.9.2.2}{}}
\abx@aux@segm{0}{0}{Zhang2000}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Stochastic Gradient Descent\nobreakspace  {}(SGD)}{86}{subsection.9.2.2}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Comparison of SGD and GD \relax }}{87}{table.caption.84}}
\newlabel{Eq:9:3}{{9.3}{87}{Stochastic Gradient Descent~(SGD)}{equation.9.2.3}{}}
\BKM@entry{id=41,dest={73656374696F6E2E392E33},srcline={255}}{4D6F6D656E74756D2D6261736564204D6574686F64}
\abx@aux@cite{Duchi2001}
\abx@aux@segm{0}{0}{Duchi2001}
\abx@aux@cite{RMSProp}
\abx@aux@segm{0}{0}{RMSProp}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.3}Momentum-based Method}{89}{section.9.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Heavy-Ball Method}{89}{subsection.9.3.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Adaptive Gradient methods\nobreakspace  {}(AdaGrad)\nobreakspace  {}\citep {Duchi2001}}{89}{subsection.9.3.2}}
\abx@aux@segm{0}{0}{kingma-adam}
\abx@aux@cite{2018on}
\abx@aux@segm{0}{0}{2018on}
\abx@aux@cite{chen2018on}
\abx@aux@segm{0}{0}{chen2018on}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}RMS-Prop\nobreakspace  {}\citep {RMSProp}}{90}{subsection.9.3.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Adam\nobreakspace  {}\citep {kingma-adam}}{90}{subsection.9.3.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Bibliography}{90}{paragraph*.85}}
\BKM@entry{id=42,dest={73656374696F6E2E392E34},srcline={367}}{4E6F6E636F6E766578206E6F6E636F6E63617665206D696E696D6178206F7074696D697A6174696F6E}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Summary}{91}{paragraph*.86}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Variants of Adam algorithm\relax }}{91}{figure.caption.87}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.4}Nonconvex nonconcave minimax optimization}{91}{section.9.4}}
\BKM@entry{id=43,dest={4974656D2E3838},srcline={35}}{417070656E6469636573}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{Appendices}{93}{Item.88}}
\BKM@entry{id=44,dest={617070656E6469782A2E3838},srcline={2}}{426173696320416C676F726974686D7320666F72204E6F6E6C696E6561722050726F6772616D6D696E67}
\BKM@entry{id=45,dest={73656374696F6E2E416C7068302E31},srcline={4}}{4772616469656E7420416C676F726974686D73}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{Basic Algorithms for Nonlinear Programming}{95}{appendix*.88}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {.1}Gradient Algorithms}{95}{section.Alph0.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.1.1}Preliminaries: convergence analysis}{95}{subsection.Alph0.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.1.2}The (Sub)gradient algorithm for Unconstrained Optimization}{96}{subsection.Alph0.1.2}}
\@writefile{loa}{\defcounter {refsection}{0}\relax }\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces The (Sub)gradient Algorithm\relax }}{96}{algorithm.1}}
\newlabel{alg:GA}{{1}{96}{The (Sub)gradient Algorithm\relax }{algorithm.1}{}}
\newlabel{code:GA}{{1}{96}{The (Sub)gradient Algorithm\relax }{ALC@unique.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Worst Case Bounds}{96}{paragraph*.89}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.1.3}Gradient Algorithm with Exact Line-Search}{97}{subsection.Alph0.1.3}}
\newlabel{The:5:1}{{.2}{97}{}{theorem.Alph0.1.2}{}}
\newlabel{Eq:5:1}{{4}{97}{}{equation.Alph0.1.4}{}}
\newlabel{Eq:5:2}{{5}{97}{Gradient Algorithm with Exact Line-Search}{equation.Alph0.1.5}{}}
\newlabel{Eq:5:5}{{8}{98}{Gradient Algorithm with Exact Line-Search}{equation.Alph0.1.8}{}}
\newlabel{Eq:5:6}{{9}{98}{Gradient Algorithm with Exact Line-Search}{equation.Alph0.1.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.1.4}Gradient Algorithm with Diminishing Step Sizes}{99}{subsection.Alph0.1.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.1.5}Gradient Algorithm with Armijo's Rule}{100}{subsection.Alph0.1.5}}
\newlabel{Eq:5:7:a}{{10a}{100}{Gradient Algorithm with Armijo's Rule}{equation.Alph0.1.1}{}}
\newlabel{Eq:5:7:b}{{10b}{100}{Gradient Algorithm with Armijo's Rule}{equation.Alph0.1.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.1.6}The Gradient Algorithm for non-strongly convex case}{101}{subsection.Alph0.1.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.1.7}Linear Convergence without Second Order Differentiability}{101}{subsection.Alph0.1.7}}
\BKM@entry{id=46,dest={73656374696F6E2E416C7068302E32},srcline={285}}{5468652050757265204E6577746F6E2773204D6574686F64}
\newlabel{Eq:5:8}{{11}{102}{Linear Convergence without Second Order Differentiability}{equation.Alph0.1.11}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {.2}The Pure Newton's Method}{102}{section.Alph0.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Motivation}{103}{paragraph*.90}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.2.1}Local Convergence Analysis}{104}{subsection.Alph0.2.1}}
\BKM@entry{id=47,dest={73656374696F6E2E416C7068302E33},srcline={408}}{50726163746963616C20496D706C656D656E746174696F6E206F66204E6577746F6E2773206D6574686F64}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {.3}Practical Implementation of Newton's method}{105}{section.Alph0.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.3.1}Cholesky Factorization}{105}{subsection.Alph0.3.1}}
\@writefile{loa}{\defcounter {refsection}{0}\relax }\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Basic Cholesky factorization Algorithm\relax }}{106}{algorithm.2}}
\newlabel{alg:CFA}{{2}{106}{Basic Cholesky factorization Algorithm\relax }{algorithm.2}{}}
\newlabel{code:GA}{{1}{106}{Basic Cholesky factorization Algorithm\relax }{ALC@unique.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.3.2}Modified Newton's method}{107}{subsection.Alph0.3.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.3.3}The Trust Region Approach}{107}{subsection.Alph0.3.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {.3.4}Implementation of Least Squares Problem}{107}{subsection.Alph0.3.4}}
\newlabel{LastPage}{{}{108}{}{page.108}{}}
\xdef\lastpage@lastpage{108}
\xdef\lastpage@lastpageHy{108}
