@ARTICLE{arvolumeonly,
    author = {ArAuthor, A. and Author, B. and Author, C.},
     title = {An article on something interesting (volume only)},
   journal = {Journal of interesting Things},
    volume = {6},
      year = {2014},
     pages = {1--122},
      issn = {0899-8256},
       doi = {10.5161/202.00000013},
     note  ={I have something special to say about this publication},
     url ={http://www.nowpublishers.com/},
     urldate={2014-07-10}
}

@Book{yang17monograph,
  author = 	 {Shenghao Yang and Raymond Yeung},
  title = 	 {{BATS} Codes: Theory and Practice},
  series = {Synthesis Lectures on Communication Networks},
  publisher = 	 {Morgan \& Claypool Publishers},
  year = 	 {2017},
  doi = {10.2200/S00794ED1V01Y201708CNT019},
}

@inbook{ber1995,
author = {Bertsekas, Dimitri},
year = {1995},
month = {01},
pages = {},
title = {Dynamic Programming and Optimal Control},
volume = {1}
}

@article{Sutton1988,
  title={Learning to predict by the methods of temporal differences},
  author={Richard S. Sutton},
  journal={Machine Learning},
  year={1988},
  volume={3},
  pages={9-44}
}
@INPROCEEDINGS{Watkins92,
    author = {Christopher J. C. H. Watkins and Peter Dayan},
    title = {Q-learning},
    booktitle = {Machine Learning},
    year = {1992},
    pages = {279--292}
}



@INPROCEEDINGS{Glorot10understandingthe,
    author = {Xavier Glorot and Yoshua Bengio},
    title = {Understanding the difficulty of training deep feedforward neural networks},
    booktitle = {In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS?10). Society for Artificial Intelligence and Statistics},
    year = {2010}
}

@incollection{NIPS2018_7338,
title = {How to Start Training: The Effect of Initialization and Architecture},
author = {Hanin, Boris and Rolnick, David},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {571--581},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7338-how-to-start-training-the-effect-of-initialization-and-architecture.pdf}
}

@incollection{NIPS2016_6322,
title = {Exponential expressivity in deep neural networks through transient chaos},
author = {Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {3360--3368},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos.pdf}
}

@Book{Bill86,
  Title                    = {Probability and Measure},
  Author                   = {Patrick Billingsley},
  Publisher                = {John Wiley and Sons},
  Year                     = {1986},
  Edition                  = {Second}
}






@InProceedings{pmlr-v80-xiao18a,
  title = 	 {Dynamical Isometry and a Mean Field Theory of {CNN}s: How to Train 10,000-Layer Vanilla Convolutional Neural Networks},
  author = 	 {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel and Pennington, Jeffrey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5393--5402},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmassan, Stockholm Sweden},
  month = 	 {10-15 Jul},
  publisher = 	 {PMLR}
}


@inproceedings{li2018on,
title={On Random Deep Weight-Tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, and Implications to Training},
author={Ping Li and Phan-Minh Nguyen},
booktitle={International Conference on Learning Representations},
year={2019}
}
@inproceedings{DBLP,
  author    = {Jeffrey Pennington and
               Samuel S. Schoenholz and
               Surya Ganguli},
  title     = {Resurrecting the sigmoid in deep learning through dynamical isometry:
               theory and practice},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, 4-9 December 2017,
               Long Beach, CA, {USA}},
  pages     = {4785--4795},
  year      = {2017}
}
@inproceedings{Pennington2018TheEO,
  title={The Emergence of Spectral Universality in Deep Networks},
  author={Jeffrey Pennington and Samuel S. Schoenholz and Surya Ganguli},
  booktitle={AISTATS},
  year={2018}
}
download as .bib file

@article{08987,
  author    = {Dar Gilboa and
               Bo Chang and
               Minmin Chen and
               Greg Yang and
               Samuel S. Schoenholz and
               Ed H. Chi and
               Jeffrey Pennington},
  title     = {Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs},
  journal   = {CoRR},
  volume    = {abs/1901.08987},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.08987},
  archivePrefix = {arXiv},
  eprint    = {1901.08987},
  timestamp = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-08987},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
zhang2018residual,
title={Residual Learning Without Normalization via Better Initialization},
author={Hongyi Zhang and Yann N. Dauphin and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1gsz30cKX},
}

@INPROCEEDINGS{Saxe14exactsolutions,
    author = {Andrew M. Saxe and James L. Mcclelland and Surya Ganguli},
    title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural network},
    booktitle = {In International Conference on Learning Representations},
    year = {2014}
}

@InProceedings{Wu_2018_ECCV,
author = {Wu, Yuxin and He, Kaiming},
title = {Group Normalization},
booktitle = {The European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@inproceedings{He2016res,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
year = {2016},
month = {06},
pages = {770-778},
title = {Deep Residual Learning for Image Recognition},
doi = {10.1109/CVPR.2016.90}
}


@inproceedings{
frankle2018the,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}

@misc{WinNT,
  title = {How to comment the paper "The Lottery Ticket Hypothesis"},
  howpublished = {\url{https://www.zhihu.com/question/323214798}},
  note = {Accessed: 2019-08-14}
}

@misc{srivastava2015highway,
  abstract = {There is plenty of theoretical and empirical evidence that depth of neural
networks is a crucial ingredient for their success. However, network training
becomes more difficult with increasing depth and training of very deep networks
remains an open problem. In this extended abstract, we introduce a new
architecture designed to ease gradient-based training of very deep networks. We
refer to networks with this architecture as highway networks, since they allow
unimpeded information flow across several layers on "information highways". The
architecture is characterized by the use of gating units which learn to
regulate the flow of information through a network. Highway networks with
hundreds of layers can be trained directly using stochastic gradient descent
and with a variety of activation functions, opening up the possibility of
studying extremely deep and efficient architectures.},
  added-at = {2017-08-18T11:10:31.000+0200},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
  biburl = {https://www.bibsonomy.org/bibtex/2c6605032c410f6eb205e05c99cbf1728/daschloer},
  description = {Highway Networks},
  interhash = {76706a0d80a35f11aa842480a6256d4f},
  intrahash = {c6605032c410f6eb205e05c99cbf1728},
  keywords = {deep highway nn},
  note = {cite arxiv:1505.00387Comment: 6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop.  Full paper is at arXiv:1507.06228},
  timestamp = {2017-10-04T16:28:59.000+0200},
  title = {Highway Networks},
  url = {http://arxiv.org/abs/1505.00387},
  year = 2015
}


@Article{Zhang2000,
author="Zhang, Y.
and Tapia, R.
and Velazquez, L.",
title="On Convergence of Minimization Methods: Attraction, Repulsion, and Selection",
journal="Journal of Optimization Theory and Applications",
year="2000",
month="Dec",
day="01",
volume="107",
number="3",
pages="529--546",
issn="1573-2878",
doi="10.1023/A:1026443131121",
url="https://doi.org/10.1023/A:1026443131121"
}


@inproceedings{Balduzzi2017,
 author = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, J P and Ma, Kurt Wan-Duo and McWilliams, Brian},
 title = {The Shattered Gradients Problem: If Resnets Are the Answer, then What is the Question?},
 booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
 series = {ICML'17},
 year = {2017},
 location = {Sydney, NSW, Australia},
 pages = {342--350},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3305381.3305417},
 acmid = {3305417},
 publisher = {JMLR.org},
} 
@inproceedings{Szegedy2016Inceptionv4IA,
  title={Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
  author={Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke},
  booktitle={AAAI},
  year={2016}
}

@inproceedings{glorot2011relu,
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Y},
year = {2010},
month = {01},
pages = {},
title = {Deep Sparse Rectifier Neural Networks},
volume = {15},
journal = {Journal of Machine Learning Research}
}

@inproceedings{DDR2919332,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
 booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
 series = {ICCV '15},
 year = {2015},
 isbn = {978-1-4673-8391-2},
 pages = {1026--1034},
 numpages = {9},
 url = {http://dx.doi.org/10.1109/ICCV.2015.123},
 doi = {10.1109/ICCV.2015.123},
 acmid = {2919814},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@InProceedings{pmlr-v49-lee16,
  title = 	 {Gradient Descent Only Converges to Minimizers},
  author = 	 {Jason D. Lee and Max Simchowitz and Michael I. Jordan and Benjamin Recht},
  booktitle = 	 {29th Annual Conference on Learning Theory},
  pages = 	 {1246--1257},
  year = 	 {2016},
  editor = 	 {Vitaly Feldman and Alexander Rakhlin and Ohad Shamir},
  volume = 	 {49},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Columbia University, New York, New York, USA},
  month = 	 {23--26 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v49/lee16.pdf},
  url = 	 {http://proceedings.mlr.press/v49/lee16.html},
  abstract = 	 {We show that gradient descent converges to a local minimizer, almost surely with random initial- ization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.}
}

@misc{ISIT2019,
  title = {Understanding nonconvex optimization},
  howpublished = {\url{http://praneethnetrapalli.org/UnderstandingNonconvexOptimization-V5.pdf}},
  note = {Accessed: 2019-08-18}
}

@article{BALDI198953,
title = "Neural networks and principal component analysis: Learning from examples without local minima",
journal = "Neural Networks",
volume = "2",
number = "1",
pages = "53 - 58",
year = "1989",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(89)90014-2",
url = "http://www.sciencedirect.com/science/article/pii/0893608089900142",
author = "Pierre Baldi and Kurt Hornik",
keywords = "Neural networks, Principal component analysis, Learning, Back propagation",
abstract = "We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed."
}

@incollection{NIPS2016_6112,
title = {Deep Learning without Poor Local Minima},
author = {Kawaguchi, Kenji},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {586--594},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf}
}

@incollection{NIPS19951028,
title = {Exponentially many local minima for single neurons},
author = {Peter Auer and Herbster, Mark and Warmuth, Manfred K},
booktitle = {Advances in Neural Information Processing Systems 8},
editor = {D. S. Touretzky and M. C. Mozer and M. E. Hasselmo},
pages = {316--322},
year = {1996},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1028-exponentially-many-local-minima-for-single-neurons.pdf}
}

@incollection{NIPS2015_5784,
title = {Learning both Weights and Connections for Efficient Neural Network},
author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {1135--1143},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf}
}


@ARTICLE{410380, 
author={ {Xiao-Hu Yu} and {Guo-An Chen}}, 
journal={IEEE Transactions on Neural Networks}, 
title={On the local minima free condition of backpropagation learning}, 
year={1995}, 
volume={6}, 
number={5}, 
pages={1300-1303}, 
keywords={backpropagation;feedforward neural nets;minimisation;local minima free condition;backpropagation learning;noncoincident input patterns;two-layered feedforward neural network;sigmoidal hidden neuron;dummy hidden neuron;suboptimal equilibrium point;error surface;local minima;Neurons;Neural networks;Backpropagation algorithms;Feedforward neural networks;Supervised learning;Multi-layer neural network;Training data;Binary sequences;Communication channels;Sufficient conditions}, 
doi={10.1109/72.410380}, 
ISSN={1045-9227}, 
month={Sep.},}


@unknown{ruoyusun2018,
author = {Li, Dawei and Ding, Tian and Sun, Ruoyu},
year = {2018},
month = {12},
pages = {},
title = {Over-Parameterized Deep Neural Networks Have No Strict Local Minima For Any Continuous Activations}
}

@inproceedings{43404,
title	= {Qualitatively Characterizing Neural Network Optimization Problems},
author	= {Ian Goodfellow and Oriol Vinyals and Andrew Saxe},
year	= {2015},
URL	= {http://arxiv.org/abs/1412.6544},
booktitle	= {International Conference on Learning Representations}
}


@incollection{NIPS2018_8095,
title = {Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs},
author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8789--8798},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8095-loss-surfaces-mode-connectivity-and-fast-ensembling-of-dnns.pdf}
}


@unknown{Gotmare2018,
author = {Gotmare, Akhilesh and Shirish Keskar, Nitish and Xiong, Caiming and Socher, Richard},
year = {2018},
month = {06},
pages = {},
title = {Using Mode Connectivity for Loss Landscape Analysis}
}

@article{DBLPkarol,
  author    = {Karol Kurach and
               Mario Lucic and
               Xiaohua Zhai and
               Marcin Michalski and
               Sylvain Gelly},
  title     = {The {GAN} Landscape: Losses, Architectures, Regularization, and Normalization},
  journal   = {CoRR},
  volume    = {abs/1807.04720},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.04720},
  archivePrefix = {arXiv},
  eprint    = {1807.04720},
  timestamp = {Mon, 13 Aug 2018 16:47:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-04720},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Hornik1991,
 author = {Hornik, Kurt},
 title = {Approximation Capabilities of Multilayer Feedforward Networks},
 journal = {Neural Netw.},
 issue_date = {1991},
 volume = {4},
 number = {2},
 month = mar,
 year = {1991},
 issn = {0893-6080},
 pages = {251--257},
 numpages = {7},
 url = {http://dx.doi.org/10.1016/0893-6080(91)90009-T},
 doi = {10.1016/0893-6080(91)90009-T},
 acmid = {109700},
 publisher = {Elsevier Science Ltd.},
 address = {Oxford, UK, UK},
} 

%%
@electronic{jiewang2019,
author = {Jie Wang},
title = {MAT3006: Real Analysis; Lecture 8},
year = {2019},
note={Available at the link \url{https://walterbabyrudin.github.io/information/Updates/MAT3006/Week4_Wednesday.pdf}},
}

@electronic{jiewang20191,
author = {Jie Wang},
title = {MAT2006: Elementary Real Analysis},
year = {2019},
note={Available at the link \url{https://walterbabyrudin.github.io/information/Notes/MAT2006.pdf}},
}

@article{3561150,
  abstract = {{In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.}},
  added-at = {2012-03-02T03:39:18.000+0100},
  author = {Cybenko, G.},
  biburl = {https://www.bibsonomy.org/bibtex/2be85c56ae384216b2e35bdf79b7fb477/baby9992006},
  citeulike-article-id = {3561150},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/BF02551274},
  citeulike-linkout-1 = {http://www.springerlink.com/content/n873j15736072427},
  day = 1,
  doi = {10.1007/BF02551274},
  interhash = {96aecb02daa11041489259a8edb54070},
  intrahash = {be85c56ae384216b2e35bdf79b7fb477},
  issn = {0932-4194},
  journal = {Mathematics of Control, Signals, and Systems (MCSS)},
  keywords = {approximation, control, duckling, free, lunch, no, theorem, theory, ugly, universal},
  month = dec,
  number = 4,
  pages = {303--314},
  posted-at = {2012-02-28 13:17:08},
  priority = {2},
  publisher = {Springer London},
  timestamp = {2012-03-02T03:39:20.000+0100},
  title = {{Approximation by superpositions of a sigmoidal function}},
  url = {http://dx.doi.org/10.1007/BF02551274},
  volume = 2,
  year = 1989
}


@Article{Barron1994,
author="Barron, Andrew R.",
title="Approximation and estimation bounds for artificial neural networks",
journal="Machine Learning",
year="1994",
month="Jan",
day="01",
volume="14",
number="1",
pages="115--133"
}


@incollection{NIPS2018_7855,
title = {ResNet with one-neuron hidden layers is a Universal Approximator},
author = {Lin, Hongzhou and Jegelka, Stefanie},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {6169--6178},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7855-resnet-with-one-neuron-hidden-layers-is-a-universal-approximator.pdf}
}




@incollection{NIPS2014_5423,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {2672--2680},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
}


@inproceedings{43405,
title	= {Explaining and Harnessing Adversarial Examples},
author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
year	= {2015},
URL	= {http://arxiv.org/abs/1412.6572},
booktitle	= {International Conference on Learning Representations}
}
@inproceedings{42503,
title	= {Intriguing properties of neural networks},
author	= {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
year	= {2014},
URL	= {http://arxiv.org/abs/1312.6199},
booktitle	= {International Conference on Learning Representations}
}

@INPROCEEDINGS{7958570, 
author={N. {Carlini} and D. {Wagner}}, 
booktitle={2017 IEEE Symposium on Security and Privacy (SP)}, 
title={Towards Evaluating the Robustness of Neural Networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={39-57}, 
keywords={neural nets;security of data;neural networks;machine learning;defensive distillation;attack algorithms;distance metrics;high-confidence adversarial examples;transferability test;Neural networks;Robustness;Measurement;Speech recognition;Security;Malware;Resists}, 
doi={10.1109/SP.2017.49}, 
ISSN={2375-1207}, 
month={May},}
@inproceedings{SPA3327757,
 author = {Wong, Eric and Schmidt, Frank R. and Metzen, Jan Hendrik and Kolter, J. Zico},
 title = {Scaling Provable Adversarial Defenses},
 booktitle = {Proceedings of the 32Nd International Conference on Neural Information Processing Systems},
 series = {NIPS'18},
 year = {2018},
 location = {Montr\&\#233;al, Canada},
 pages = {8410--8419},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3327757.3327932},
 acmid = {3327932},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 



@inproceedings{Chen2017,
 author = {Chen, Pin-Yu and Zhang, Huan and Sharma, Yash and Yi, Jinfeng and Hsieh, Cho-Jui},
 title = {ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks Without Training Substitute Models},
 booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
 series = {AISec'17},
 year = {2017},
  location = {Dallas, Texas, USA},
 pages = {15--26},
 numpages = {12},
  doi = {10.1145/3128572.3140448},
 acmid = {3140448},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@InProceedings{Ilyas2018,
  title = 	 {Black-box Adversarial Attacks with Limited Queries and Information},
  author = 	 {Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2137--2146},
  year = 	 {2018},
    volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  }

@InProceedings{kingma-adam,
author = {Kingma, Diederick P and Ba, Jimmy},
title = {Adam: A method for stochastic optimization},
booktitle = { International Conference on Learning Representations (ICLR) },
year = {2015}
}

@article{Nesterov2011,
author = {Nesterov, Yu},
year = {2011},
month = {01},
pages = {},
title = {Random gradient-free minimization of convex functions}
}

@inproceedings{Razaviyayn2014SuccessiveCA,
  title={Successive Convex Approximation: Analysis and Applications},
  author={Meisam Razaviyayn},
  year={2014}
}
@article{Duchi2001,
 author = {Duchi, John and Hazan, Elad and Singer, Yoram},
 title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
 journal = {J. Mach. Learn. Res.},
 issue_date = {2/1/2011},
 volume = {12},
 month = jul,
 year = {2011},
 issn = {1532-4435},
 pages = {2121--2159},
 numpages = {39},
 url = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
 acmid = {2021068},
 publisher = {JMLR.org},
} 

@electronic{RMSProp,
author = {Tieleman},
title = {Lecture 6.5-rmsprop},
year = {2012},
note={Available at the link \url{https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}},
}
@inproceedings{2018on,
title={On the Convergence of Adam and Beyond},
author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=ryQu7f-RZ},
}

@inproceedings{chen2018on,
title={On the Convergence of A Class of Adam-Type Algorithms  for Non-Convex Optimization},
author={Xiangyi Chen and Sijia Liu and Ruoyu Sun and Mingyi Hong},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1x-x309tm},
}