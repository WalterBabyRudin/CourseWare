
\chapter{Week1}

\section{Wednesday}\index{Wednesday_lecture}

\subsection{Motivation}
To evaulate a dynamic, stochastic system,
\begin{itemize}
\item
the performance measure is not analytically tractable
\end{itemize}
Example:
\begin{itemize}
\item
Expected waiting time in an emergency room;
\item
Price of option;
\item
Probability of failure of a power grid.
\end{itemize}
Applications:
\begin{itemize}
\item
Small-sample inference.
Bootstrap, or permulation test;
\item
Non-convex Optimization.
SGD, or simulation annealing.
\end{itemize}
Basic Procedure for simulation:
\[
\bm U\to\bm X\to\bm Y
\]
with $\bm U\sim\mathcal{U}(0,1)$,
$\bm X$ denotes the input random variable with specified distribution,
and $\bm Y$ denotes the output random variable whose properties we wish to estimate.
We will mainly talk about how to generate $\bm X$ from $\bm U$.

\begin{example}
[Queuing Problem]
Consider a single-server queue with
\begin{itemize}
\item
infinite buffer size;
\item
FIFO;
\item
$A_n$: arrive time; (usually given)
\item
$D_n$: departure time, decomposed as:
\begin{itemize}
\item
$T_n=A_{n+1} - A_n$, inter-arrival time;
\item
$V_n$: service time
\end{itemize}
\item
$W_n$: waiting time (before entering service):
\[
W_n = (D_{n-1} - A_n)^+
\]
Lindley recursion:
\[
W_{n+1} = (W_n  + V_n - T_n)^+
\]
\end{itemize}
Performance measure:
\begin{itemize}
\item
Mean waiting time: $\mathbb{E}[W_\infty] = \lim_{N\to\infty}\frac{1}{N}\sum_{k=1}^NW_k$;
\item
Tail of waiting time: $P(W_\infty > x) = \lim_{N\to\infty}\frac{1}{N}\sum_{k=1}^N\bm 1\{W_k >x\}$
\end{itemize}
There is no closed-form for general distribution.
\end{example}

\begin{remark}
Simulation solution (Monte Carlo Method):
\begin{enumerate}
\item
Generate i.i.d. samples of $V_n$ and $T_n$.
\item
Apply Lindley recursion to simulate $W_\infty$.
\begin{itemize}
\item
Option 1: simulate a long sequence of $W_n$ to approximate $W_\infty$;
\item
Option 2: apply advanced method to simulate exact $W_\infty$.
\end{itemize}
\item
Repeat the simulation procedure to get multiple $W_\infty$
\item
Output the empirical distribution of simulated samples.
\end{enumerate}
\end{remark}

Performance optimization:
Denote $\mu$ as the service time.
\[
\begin{aligned}
\min_{\mu}&\quad\text{Cost}(\mu) + \mathbb{E}[W_\infty(\mu)]\\
\mbox{s.t.}&\quad W_{n+1} =\left(
W_n +\frac{V_n}{\mu} - T_n
\right)^+
\end{aligned}
\]
Or
\[
\begin{aligned}
\min_{\mu}&\quad\text{Cost}(\mu)\\
\mbox{s.t.}&\quad W_{n+1} =\left(
W_n +\frac{V_n}{\mu} - T_n
\right)^+\\
&\quad \mathbb{P}(W_\infty > x) < \varepsilon
\end{aligned}
\]

Simulation optimization:
\begin{enumerate}
\item
Generate $Z_{n}(\mu)$ by Lindley recursion such that
\[
\mathbb{E}[Z_n(\mu)] = 
\frac{\partial }{\partial \mu}
\mathbb{E}[W_\infty(\mu)]
\]
\item
Apply first order method for optimization:
\[
\mu_{k+1} = \mu_k - \delta_k\cdot\bigg(
\mathbb{E}[Z_n(\mu)]+\nabla_{\mu}\text{Cost}(\mu)
\bigg)
\]
\end{enumerate}
\paragraph{Key Issues}
\begin{enumerate}
\item
How to generate the needed random variables?
\item
How to compute the limiting stationary distribution?
\item
How to estimate the sensitivity?
\item
How to use simulation to optimize?
\end{enumerate}

Computational Efficiency:
\begin{enumerate}
\item
The computational cost to obtain a good numerical solution;
\item
How to exploit the problem structure to speed up the computation?
\end{enumerate}

\subsection{Generating Random Variables}

We first discuss how to simulate a scalar random variable.
\paragraph{Generating Uniform Numbers}
Two types of random number generate~(RNG):
\begin{itemize}
\item
Mathematical (Pseudo): multiple recursive generator;
\item
Physical: nuclear decay.
\end{itemize}
Multiple Recursive Generator~(MRG):
\begin{enumerate}
\item
Choose a large prime number $m$,
and $a_1,\ldots,a_k$ are integers such that the following recursion has cycle length $m^k-1$:
\[
x_i\equiv(a_1x_{i-1}+a_2x_{i-2}+\cdots+a_kx_{i-k})~(\bmod~m).
\]
Output: $U_i = (x_i,x_{i-1},\ldots,x_{i-k+1})$.
\item
Example: $k=1,m = 2^{31}-1$.
\end{enumerate}

Hot research topics in UNG:
\begin{itemize}
\item
better design of UNG;
\item
test whether the generated sequence is i.i.d.;
\item
measrue the performance of a generator.
\end{itemize}

In most of the analysis, we assume that $\bm U$ is given.

\paragraph{Transfer RN to RV}
\begin{itemize}
\item
Distribution function: $F_X(x) = \mathbb{P}(X\le x)$.
\item
Inverse function: $F_X^{-1}(u) = \inf\{x:~F(x)\ge u\}$.
\end{itemize}

\begin{theorem}[Inverse Method]
Let $U\sim\mathcal{U}(0,1)$, and $F_X(x)$ the distribution function of $X$,
then $F_X^{-1}(U)$ follows the same distribution of $X$.
\end{theorem}
\begin{proof}
It suffices to show that $F^{-1}_X(U)\stackrel{d}{=}X$.
It suffices to check their cdf are the same:
\begin{align*}
\mathbb{P}(F^{-1}(U)\le x)&=
\mathbb{P}(\inf\{y: F(y)\ge U\}\le x)\\
&=\mathbb{P}(F(x)\ge U)=F(x)
\end{align*}
\end{proof}

\begin{example}
Then we discuss how to generate i.i.d. exponential random variables with rate $\mu$:
\[
F(x) = 1 - e^{-\mu x}\implies
F^{-1}(u) = -\log (1-u)/\mu.
\]
Therefore, we can first generate $U$, and then output
\[
X = -\log (1-U)/\mu\triangleq -\log U/\mu
\]
\end{example}

\begin{example}
The generation of i.i.d. discrete random variables is a little bit tricky:
\[
\mathbb{P}(X=k) = p_k,\quad
k=1,2,3,\ldots.
\]
\end{example}

Sometimes we don't know the analytical form of the inverse distribution function, such as the normal distribution.
Then statisticans consider the accpetance-rejection method.

Circle example.

We apply the similar idea to two distributions.
We have a simple distribution $g(x)$ and a complicated distribution $f(x)$.
First find $c>0$ s.t. $f(x)\le g(x)$.
\begin{enumerate}
\item
Generate $Y\sim g(x)$, which is our $x$-location;
\item
Generate $U\sim\mathcal{U}(0,1)$.
When $U\le f(x)/(cg(x))$, stop and return $X$;
otherwise discard $X$ and repeat step 1.
\end{enumerate}

Here $f$ is called the \emph{target distribution}, and $g$ the \emph{proposed distribution}, the probability $\mathbb{E}_{x\sim g(x)}[f(x)/c(g(x))]$ is called the acceptance rate:
\[
\mathbb{E}_{x\sim g(x)}[f(x)/c(g(x))] = \frac{1}{c}.
\]
\begin{example}
Consider the Beta distribution with density
\[
f(x) = x^{\alpha  - 1} (1-x)^{\beta - 1}/B(\alpha,\beta),\quad
x\in[0,1],\alpha,\beta>1.
\]
Since it is not easy to find the inverse distribution function, we select the proposed distribution $g\sim\mathcal{U}(0,1)$.
We take $C = \max_{x\in[0,1]}f(x)$:
\[
C = \max_{x\in[0,1]}
x^{\alpha  - 1} (1-x)^{\beta - 1}/B(\alpha,\beta)
\le
\frac{1}{B(\alpha,\beta)}
\]
A simpler choice is $C = \frac{1}{B(\alpha,\beta)}$.
The procedure is as follows:
\begin{enumerate}
\item
Generate $U_1,U_2\sim\mathcal{U}(0,1)$.
\item
Compute the ratio $L =\frac{f(U_1)}{cg(U_1)} = U_1^{\alpha  - 1} (1-U_1)^{\beta - 1}$.
\end{enumerate}
\end{example}

\begin{example}
Then we discuss how to generate normal distributions.
Set $g(x) = ue^{-ux}$ and $f(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^2}{2}\right)$.
As a result,
\[
C = \max_{0\le x<\infty}\frac{1}{\sqrt{2\pi}\mu}\exp\left(
-\frac{x^2}{2} + ux
\right)
=
\frac{1}{\sqrt{2\pi}\mu}\exp\left(
\frac{u^2}{2}
\right)
\]
with $u^* = \arg\min(\frac{1}{\sqrt{2\pi}\mu} e^{u^2/2})$.
The ratio
\[
L=\exp\left(
-\frac{x^2}{2} + u^*x - \frac{(u^*)^2}{2}
\right)
\]

\end{example}
General rule: use a heavy tail distribution function.

\paragraph{More accpetance-rejection ideas}
\[
f(x)\propto \tilde{f}(x).
\]
Suppose that we can generate from density $h(x)$,
\[
c^* = \max\frac{\tilde{f}(x)}{h(x)}
\]
\paragraph{Squeezing}
Suppose that the density $f(x)$ is difficult to evaluate.
Suppose there exists two sequences of functions $h_n(x),g_n(x)$ such that
\begin{itemize}
\item
$g_n(x)$ is a density and $f(x)\le cg_n(x)$ for a fixed $c,\forall n$.
\item
$h_n(x)\le f(x),\forall n$
\item
$g_n(x),h_n(x)\to f(x)$ pointwisely as $n\to\infty$.
\end{itemize}
\[
\frac{\hat{h}_n(x)}{cg(x)}
<
\frac{f(x)}{cg(x)}
<
\frac{\bar{h}_n(x)}{cg(x)}
\]
When $U<\frac{\hat{h}_n(x)}{cg(x)}$ or $U>\frac{\bar{h}_n(x)}{cg(x)}$;
otherwise, $n\leftarrow n+1$.







