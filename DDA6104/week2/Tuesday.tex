
\chapter{Week2}

\section{Wednesday}\index{Wednesday_lecture}

\paragraph{Box Muller Method}
When $x_1,x_2\sim \mathcal{N}(0,1)$, we find 
\[
r^2=x_1^2+x_2^2\sim\mathcal{X}^2,\qquad
\theta\sim\mathcal{U}(0,1).
\]
It follows that
\[
\begin{array}{ll}
Y_1\leftarrow \sqrt{-2\log U_1}\sin(2\pi U_2),
&
Y_2\leftarrow \sqrt{-2\log U_1}\cos(2\pi U_2).
\end{array}
\]


\paragraph{Alias}
Let $X\in\{1,2,\ldots,n\}$.
The pre-computation process is to find $n$ pairs $(x_{1,\ell},x_{2,\ell}), (p_{1,\ell},p_{2,\ell})$ for $\ell\in[n]$ s.t.
\[
\sum_{\ell}\sum_{i=1}^2p_{i,\ell}\bm1\{x_{i,\ell}=k\}
=
np_k,\quad
k\in[n].
\]
These $n$ pairs could be updated simply.
Then we can view the distribution of $X$ as a mixture of $n$ two-point distributions.
\begin{itemize}
\item
Generate a uniform $\ell\in[n]$;
\item
Generate a two point distribution s.t. $P(X=x_{i,\ell})=p_{i,\ell}$;
\item
Return a sample for $X$.
\end{itemize}

\subsection{Generating Multi-variate Random Variables}
Simulating a multivariate normal distribution is simple.
The random variable $\bm X\sim\mathcal{N}(0,\bm \Sigma)$ can be obtained by
\[
\bm X=\bm\Sigma^{1/2}\bm X_0,\quad
\mbox{with }\bm X_0\sim\mathcal{N}(0,\bm I)
\]
The $2$-dimension random variable can be obtained by
\[
\left\{
\begin{aligned}
X_1&=\sigma_1Z_1\\
X_2&=\rho \sigma_2Z_1 + \sigma_2\sqrt{1-\rho^2}Z_2
\end{aligned}
\right.
\]
One way is to decompose the covariance matrix by Cholesky factorization.


\paragraph{Multi-nominal Distribution}
\begin{itemize}
\item
Generate $X_1\sim\mbox{binomial}(N,p_1)$
\item
Generate $X_2\sim\mbox{binomial}(N-X_1,p_2/(1-p_1))$
\item
Keep the remaining proceed.
\end{itemize}

The idea is to sample the marginal distribution $X_1$, then sample the conditional distribution of $X_i$ given $X_{1:i-1}$.

\paragraph{Copula}
Characterize the dependence structure.
\begin{enumerate}
\item
Generate a coupla $U=(U_1,\ldots,U_p)$
\item
Compute $X_i = F_i^{-1}(U_i)$.
\end{enumerate}

\subsection{Simple Stochastic Process}

Homogeneous Possion Process:
$N(t)\sim\text{PP}(\beta)$ for $0\le t\le T$.
\begin{itemize}
\item
Use the inter-arrival time.
\item
First generate $N(T)\sim\text{Possion}(\beta T)$; then generate $N(t)$ uniformly at $[0,T]$.
\end{itemize}


Inhomogeneous Possion Process:
$N(t)\sim\text{PP}(\beta(t))$ for $\beta(t)\le\beta$.
By acceptance-rejection method, or
\begin{itemize}
\item
$N(T)\sim\text{Possion}(\int_0^T\beta(t)\diff t)$;
\item
Generate $N(t)$ random variables by $c\beta(t)$ on $[0,T]$
\end{itemize}
The advantage of the second method is that it is easy to generalize into $[0,\infty)$.


\paragraph{Continuous Time Markov Chain}
Let $J(t)$ be a Markov process with intensity matrix $\Lambda$.
\begin{itemize}
\item
Simulate the holding time $-\lambda_{i,i}$
\item
Decide which state he aim to jump. For state $j$, w.p. $\lambda_{i,j}/-\lambda_{i,i}$.
\end{itemize}


\section{Output Analysis}

\begin{enumerate}
\item
Make inference;
\item
Access the performance of a simulation algorithm.
\end{enumerate}

\paragraph{Normal Confidence Interval}
We can use the sample average to approximate the expectation.
\[
\sqrt{N}(\hat{z} - z)\to \mathcal{N}(0,\text{Var}(z))
\]
Then we can construct a $1-\alpha$ confidence interval.
\[
(\hat{z} - \phi_{1-\alpha/2}\frac{\sigma}{\sqrt{N}},
\hat{z} + \phi_{1-\alpha/2}\frac{\sigma}{\sqrt{N}}
).
\]















