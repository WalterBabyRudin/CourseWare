%\chapter{Introduction to Linear Programming}
\chapter{Primal-Dual Interior Point Methods (PDIPM)}

\section{PDIPM for Linear Programming}
Consider a linear programming problem
\begin{equation}
\begin{array}{ll}
\min&\bm c\trans\bm x\\
\mbox{such that}&\bm{Ax}=\bm b\\
&\bm x\ge0
\end{array}\qquad
(P)
\end{equation}
with its dual problem
\begin{equation}
\begin{array}{ll}
\max&\bm b\trans\bm y\\
\mbox{such that}&\bm{A}\trans\bm y+\bm s=\bm c\\
&\bm s\ge0
\end{array}\qquad
(D)
\end{equation}

We assume that the \emph{primal-dual slater condition} (what is it?) holds. Consider solving the barriered problem of $(P)$ for $\mu>0$:
\begin{equation}
\begin{array}{ll}
\min&\bm c\trans\bm x - \mu \sum_{i=1}^n\ln x_i\\
\mbox{such that}&\bm{Ax}=\bm b\\
\end{array}\qquad
(P_\mu)
\end{equation}

\subsection{Duality Gap}
Let $\bm x(\mu)$ be the optimal solution for $(P_\mu)$. By KKT condition, there exists $\bm y(\mu)\in\mathbb{R}^m$ such that
\[
\bm c - \mu\bm x(\mu)^{-1} - \bm A\trans\bm y(\mu) = \bm0.
\]
Define the slack variable $\bm s(\mu) : = \mu\bm x(\mu)^{-1} = \bm c -\bm A\trans\bm y(\mu)$. It's clear that:
\begin{enumerate}
\item
$\bm x(\mu)$ is primal-feasible to $(P_\mu)$; while $(\bm y(\mu),\bm s(\mu))$ is dual-feasible to $(D_\mu)$
\item
The duality gap between $\bm x(\mu)$ and $(\bm y(\mu),\bm s(\mu))$ is
\[
\bm x(\mu)\trans \bm s(\mu) = n\mu.
\]
\end{enumerate}

\subsection{Convergence of Barrier Problem $P_\mu$}

The set $\{\bm x(\mu)\mid\mu>0\}$ is known as the \emph{primal analytic central path}; and the set
$\{(\bm y(\mu),\bm s(\mu))\mid\mu>0\}$ is known as the \emph{dual analytic central path}.
\begin{proposition}
The set $\{\bm x(\mu)\mid 0<\mu\le 1\}$ and $\{(\bm y(\mu),\bm s(\mu))\mid 0<\mu\le 1\}$ are bounded.
\end{proposition}
\begin{proof}
Let $\tilde{\bm x}$ be an interior for $(P)$ and $(\tilde{\bm y},\tilde{\bm s})$ be an interior for $(D)$. Then $\tilde{\bm x} - \bm x(\mu)\in\mathcal{N}(\bm A)$ and $\tilde{\bm s} - \bm s(\mu)\in\text{Range}(\bm A\trans)$, which implies
\[
0 = (\tilde{\bm x} - \bm x(\mu))\trans (\tilde{\bm s} - \bm s(\mu))
=\tilde{\bm x}\trans\tilde{\bm s} - \tilde{\bm x}\trans\bm s(\mu) -\tilde{\bm s}\trans\bm x(\mu) + n\mu.
\]
Therefore, $\{\bm x(\mu)\mid 0<\mu\le 1\}$ and $\{(\bm y(\mu),\bm s(\mu))\mid 0<\mu\le 1\}$ must be bounded. (question: how to specify?)
\end{proof}

\begin{proposition}
The set $\{(\bm x(\mu),\bm y(\mu),\bm s(\mu))\mid 0<\mu\le 1\}$ converges to $(\hat{\bm x},\hat{\bm y},\hat{\bm s})$ as $\mu\to0$. Moreover, the active sets $B:=\{i\mid\hat x_i>0\}$ and $N=\{j\mid \hat{s}_j>0\}$ form a partition of $\{1,\dots,n\}$.
\end{proposition}

\begin{proof}
\begin{itemize}
\item
Due to the boundness of $\{(\bm x(\mu),\bm y(\mu),\bm s(\mu))\mid 0<\mu\le 1\}$, we imply there exists a subsequence $\mu_k$ such that
\[
\lim_{k\to\infty}(\bm x(\mu),\bm y(\mu),\bm s(\mu)) = (\hat{\bm x},\hat{\bm y},\hat{\bm s})
\]
It's clear that $\hat{\bm x}$ is primal optimal and $(\hat{\bm y},\hat{\bm s})$ is dual optimal. By complementarity condition, $B\cap N=\emptyset$.
\item
At the same time, consider the equality again that
\[
0 = (\hat{\bm x} - \bm x(\mu_k))\trans(\hat{\bm s} - \bm s(\mu_k)) = -\sum_{i\in B}\hat x_is_i(\mu_k)
-
\sum_{j\in N}\hat s_jx_j(\mu_k) + n\mu_k.
\]
Or equivalently,
\[
n = \sum_{i\in B}\frac{\hat x_i}{x_i(\mu_k)}+\sum_{j\in N}\frac{\hat s_j}{s_j(\mu_k)}.
\]
Taking $k\to\infty$, we imply $|B|+|N| = n$, i.e., $B$ and $N$ form a partition of $\{1,\dots,n\}$.
\item
Next we show that $\hat{\bm x}$ is unique. We shall show that $\hat{\bm x}_B$ is the optimal solution to 
\[
\begin{array}{ll}
\max&\sum_{i\in B}\ln x_i\\
\mbox{such that}&\bm A_B\bm x_B=\bm b
\end{array}\qquad
(O)
\]
and then the uniqueness of $\hat{\bm x}_B$ is proved due to the strict concavity of the objective function. (question: why the uniqueness of $\hat{\bm x}_B$ implies the uniqueness of $\hat{\bm x}$?)

The KKT condition for $(O)$ gives
\[
\begin{array}{ll}
\bm x_B^{-1}\in\text{Range}(\bm A_B\trans)
&
\bm A_B\bm x_B=\bm b
\end{array}
\]
We may verify that $\hat{\bm x}_B$ satisfies the condition above.
\end{itemize}
\end{proof}
\begin{remark}
The particular optimal solution $\bm x$, denoted by $\bm x(0)$ is called the \emph{analytic center of the optimal face}. If taking $\mu\to\infty$, then $\bm x(\mu)$ converges to the optimal solution of
\[
\begin{array}{ll}
\max&\sum_{i=1}^n\ln x_i\\
\mbox{such that}&\bm A_B\bm x_B=\bm b
\end{array}
\]
which is known as the \emph{analytic center of the feasible region}.

Note that $\bm x(0)+\bm s(0)>0$, i.e., thery are \emph{strictly complementary}.
\end{remark}
\subsection{Central Path Following Algorithm}
\begin{theorem}[Interior Decomposition Theorem]
Suppose that (P) and (D) both satisfy the Slater's condition.
Let $\mathcal{F}_P$ be the feasible set for (P), and $\mathcal{F}_D$ be the feasible set for (D).
Then for any $\omega\in\mathbb{R}^n_{++}$, there exists unique $x\in\text{int}(\mathcal{F}_P)$ and $s\in\text{int}(\mathcal{F}_D)$ such that
$w=x\circ s$.
\end{theorem}
\begin{proof}
The optimal solution for the strictly convex problem
\[
\begin{array}{ll}
\min&c\trans x - \sum_{i=1}^n\omega_i\ln x_i\\
\text{with}&Ax=b
\end{array}
\]
satisfies the requirement.
\end{proof}

The idea for central path following algorithm is that we should approach the optimal solution by tracing the central path~(question: why the equality for all $1\le i\le n$?)
\[
x_i(\mu)s_i(\mu) = \mu, \ 1\le i\le n,
\]
as $\mu\to0$.

One type of algorithm is given below:
\begin{enumerate}
\item
Given the current primal-dual solution $(x,y,s)$, suppose our next target solution is $(x',y',s')$, and denote
\[
\begin{array}{lll}
\Delta x=x'-x,
&
\Delta y = y'-y,
&
\Delta s = s'-s
\end{array}
\]
\item
It suffices to solve the following systems in each iteration (why?) (have question on the third equality):
\begin{equation}\label{Eq:6:4}
\begin{aligned}
\left\{
\begin{aligned}
A\Delta x&=0\\
A\trans\Delta y+\Delta s&=0\\
s\circ(x'-x)+x\circ(s'-s)&=x'\circ s' - x\circ s - (x'-x)\circ(s'-s)
\end{aligned}
\right.
\end{aligned}
\end{equation}
Or equivalently,
\begin{equation}
\begin{aligned}
\left\{
\begin{aligned}
A\Delta x&=0\\
A\trans\Delta y+\Delta s&=0\\
0&= 0
\end{aligned}
\right.
\end{aligned}
\end{equation}
Since the nonlinear system is difficult to solve, we just apply Newton's method to solve it approximately in each iteration:
\begin{equation}\label{Eq:6:4}
\begin{aligned}
\left\{
\begin{aligned}
A\Delta x&=0\\
A\trans\Delta y+\Delta s&=0\\
S\Delta x + X\Delta s&= w'-w
\end{aligned}
\right.
\end{aligned}
\end{equation}
\end{enumerate}
\paragraph{Reformulation}
We define $D:=(XS^{-1})^{1/2}$ and $V:=(XS)^{1/2}$, and $d_x = D^{-1}\Delta x$, $d_y = \Delta y$, $d_s = D\Delta s$, then (\ref{Eq:6:4}) is equivalent to 
\begin{equation}\label{Eq:6:7}
\left\{
\begin{aligned}
ADd_x&=0\\
(AD)\trans d_y+d_s &= 0\\
d_x+d_s&=V^{-1}(w'-w)
\end{aligned}
\right.
\end{equation}
Why it is essentially a projection problem?

\subsection{Scaling Transformation}
The origin pair of LP can be scaled from
\[
\begin{array}{ll}
\min&c\trans x\\
\mbox{with}&Ax=b\\
&x\ge0
\end{array},\qquad
\begin{array}{ll}
\max&b\trans y\\
\mbox{with}&A\trans y+s=c\\
&s\ge0
\end{array}
\]
into
\[
\begin{array}{ll}
\min&(Dc)\trans x'\\
\mbox{with}&ADx'=b\\
&x'\ge0
\end{array},
\qquad
\begin{array}{ll}
\max&b\trans y'\\
\mbox{with}&(AD)\trans y'+s'=Dc\\
&s'\ge0
\end{array}
\]
such that
\[
\begin{array}{lll}
x'=D^{-1}x,
&
y'=y,
&
s'=Ds
\end{array}
\]
Now we answer why the (\ref{Eq:6:7}) is a projection problem. It can be rewritten as
\[
\left\{
\begin{aligned}
ADd_x&=0\\
(AD)\trans d_y+d_s &= 0\\
d_x+d_s&=d:=V^{-1}(w'-w)
\end{aligned}
\right.
\implies
\left\{
\begin{aligned}
d_x&=P_{AD}(d)\\
d_s &= (I - P_{AD})(d)
\end{aligned}
\right.
\]

\subsection{Step Size Control}
Consider the iterates
\[
\begin{array}{ll}
x(t) = x+t\Delta x,
&
s(t)=s+t\Delta s
\end{array}
\]
The idea is to make $x(t)\circ s(t)$ stay close to the central line (why?) and converge to zero.

\begin{definition}[Neighborhoods of the central line]
We define the neighborhood of the central line using either $2$-norm or $\infty$-norm:
\begin{align*}
\mathcal{N}_2(\beta)&=\left\{v^2\middle|\|1/\mu v^2 - \bm 1\|_2\le\beta\right\}\\
\mathcal{N}_\infty(\beta)&=\left\{v^2\middle|\|1/\mu v^2 - \bm 1\|_\infty\le\beta\right\}
\end{align*}
where $0<\beta<1$ is a constant and $\mu = \bm 1\trans v^2/n$.
\end{definition}

In this algorithm we choose the next target point $w'$ as
\[
w':=\rho\mu\bm 1,\ \text{where $0<\rho<1$}
\]
As a result, $d=-v+\rho\mu v^{-1}$ (question: what is $v$, and what is $v^{-1}$?).
Assume that $x\circ s = v^2\in\mathcal{N}(\beta)$, which implies
\[
v_i^2\ge(1-\beta)\mu,\ \forall i
\]
Note that 
$
x+t\Delta x>0,\quad
s+t\Delta s>0
$
iff
$
v+td_x>0,\quad
v+td_s>0
$
\paragraph{Convergence Analysis}
\begin{enumerate}
\item
\begin{align*}
\|(d_x)\circ(d_s)\|_2&\le\frac{1}{2}\|d\|_2^2\\
\|(d_x)\circ(d_s)\|_\infty&\le\frac{1}{4}\|d\|_2^2\\
\end{align*}
\item
\[
\|d\|_2^2\le\frac{1}{\sqrt{1-\beta}}\left(\|1/\mu v^2-\bm 1\|_2+(1-\rho)\sqrt{n}\right)\sqrt{\mu}
\]
This inequality can be shown using the fact that $x\circ s = v^2\in\mathcal{N}(\beta)$
\item
Note that
\begin{align*}
v(t)^2&=(x+t\Delta x)\circ(s+t\Delta s)\\
\mu(t)&=\bm 1\trans v(t)^2/n
\end{align*}
which implies
\begin{align*}
\left\|
\frac{1}{\mu(t)}v(t)^2-\bm 1
\right\|
&\le
\beta - \frac{t}{1-t+t\rho}\left[
\rho\beta - \frac{(\|v^2/\mu-\bm 1\|_2 + (1-\rho)\sqrt{n})^2}{2(1-\beta)}t
\right]\\
x(t)\trans s(t) &= (1-(1-\rho)t)x\trans s
\end{align*}
\end{enumerate}

\begin{algorithm}[htb] 
\caption{PDIPM} 
\label{alg:CFA} 
\begin{algorithmic}[1] %show number in each rows
\REQUIRE ~ %算法的输入参数：Input
Initial Guess $(x^{(0)},s^{(0)})\in\mathcal{N}(\beta)$
\STATE Let $k=0$.
\STATE Compute $\Delta x^{(k)}$ and $\Delta s^{(k)}$
\STATE Compute the maximum step length $t^*>0$ such that 
\[
(x^{(k)}+t^*\Delta x^{(k)})\circ(s^{(k)}+t^*\Delta s^{(k)})\in\mathcal{N}(\beta)
\]
\STATE Let $x^{(k+1)} = x^{(k)} + t^*\Delta x^{(k)}$; and $s^{(k+1)} = s^{(k)} + t^*\Delta s^{(k)}$.

Let $k=k+1$ and go to step 2.
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[htb] 
\caption{Algorithm for computing $\Delta x^{(k)}$ and $\Delta s^{(k)}$} 
\label{alg:CFA} 
\begin{algorithmic}[1] %show number in each rows
\REQUIRE ~ %算法的输入参数：Input
Current point $x^{(k)},s^{(k)}$
\STATE Solve the nonlinear system below approximately using Newton's method.
\[
\begin{pmatrix}
A&0&0\\
0&A\trans&I\\
S&0&X
\end{pmatrix}\begin{pmatrix}
\Delta x\\\Delta y\\\Delta s
\end{pmatrix}
=\begin{pmatrix}
0\\0\\(x^{(k)}+\Delta x)\circ(s^{(k)}+\Delta s) - x^{(k)}\circ s^{(k)} - \Delta x\circ\Delta s
\end{pmatrix}
\]
\end{algorithmic}
\end{algorithm}

Question: another version of pdipm:
\[
\begin{pmatrix}
A&0&0\\
0&A\trans&I\\
S&0&X
\end{pmatrix}\begin{pmatrix}
\Delta x\\\Delta y\\\Delta s
\end{pmatrix}
=\begin{pmatrix}
r_p\\r_d\\r_c
\end{pmatrix}
\]
where $r_p,r_d,r_c$ denotes the residuals for primal, dual, and complementarity.


















