%\chapter{Introduction to Linear Programming}
\chapter{Reviewing}
\section{KKT condition}
\begin{definition}[Tangent Cone]
Given a subset $\mathcal{X}\subseteq\mathbb{R}^n$ and $\hat{x}\in \mathcal{X}$, the \emph{tangent cone} of $\mathcal{X}$ at $\hat{x}$ is given by:
\[
\mathcal{T}(\hat{x})=\{0\}\cup\left\{
y\ne0
\middle|
\exists x^k\in\mathcal{X}: x^k\to\hat{x},\ \&\frac{x^k-\hat{x}}{\|x^k - \hat{x}\|}\to\frac{y}{\|y\|}
\right\}
\]
\end{definition}

\begin{proposition}
A vector $y$ is tangent of $X$ at $\hat{x}$ iff exists $\{x_k\}\subseteq X$ with $x_k\to\hat{x}$ and positive $\{\alpha_k\}$ such that 
\[
\alpha_k\to0, \frac{x_k-x}{\alpha_k}\to y.
\]
\end{proposition}
\begin{proof}
Let $y$ be tangent of $X$ at $\hat{x}$. For $y=0$, take $x_k=\hat{x}$ and $\alpha_k=1/k$; otherwise take $\{x_k\}$ be the same as in the definition of tangent cone, and $\alpha_k = \|x_k-\hat{x}\|/\|y\|$.

Conversely, if $y=0$, then $y$ is tangent; otherwise, we imply $x_k\to \hat{x}$ and
\[
\frac{x_k-\hat{x}}{\|x_k-\hat{x}\|}=\frac{(x_k-\hat{x})/\alpha_k}{\|(x_k-\hat{x})/\alpha_k\|}\to\frac{y}{\|y\|}
\]
\end{proof}

\begin{proposition}
Suppose that $x^*$ is a local minimum for closed set $\mathcal{X}$, then 
\[
\nabla f(x^*)\in(\mathcal{T}(x^*))^*,\Longleftrightarrow
\nabla\trans f(x^*)y\ge0,\forall y\in\mathcal{T}(x^*)
\]
\end{proposition}
\begin{proof}
Suppose that $y$ is a nonzero tangent of $X$ at $x^*$, then exists $\{\xi^k\}$ and $\{x^k\}\subseteq X$ with $x^k\ne x^*$ such that
\[
\xi^k\to 0,x^k\to x^*,\frac{x^k-x^*}{\|x^k-x^*\|}=\frac{y}{\|y\|}+\xi^k
\]
By MVT, $f(x^k) = f(x^*) + \nabla f(\tilde{x}^k)\trans(x^k-x^*)$, i.e.,
\[
f(x^k) =  f(x^*) + \frac{\|x^k-x^*\|}{\|y\|}\nabla f(\tilde{x}^k)\trans y^k,
\]
with $y^k = y + \|y\|\xi^k$. Suppose on the contrary that $y$ is a descent dircetion of $f$ at $x^*$, i.e., $\nabla f(x^*)\trans y<0$, then argue that $\nabla f(\tilde{x}^k)\trans y^k<0$, which follows that $f(x^k)<f(x^*)$.
\end{proof}
\begin{corollary}
For convex $X$, all $x-x^*$ belong to $T(x^*)$, and therefore $\nabla\trans f(x^*)(x-x^*)\ge0$ for any $x\in X$.
Nondifferentiable: $d\trans (x-x^*)\ge0$, where $d$ is some vector in $\partial f(x^*)$.
\end{corollary}
\begin{example}
For the polyhedral $\mathcal{X}=\{x\mid a_i\trans x\le b_i, i=1,\dots,m, a_i\trans x = b_i, i=m+1,\dots,m+n\}$, we denote $I(\hat{x}) = \{m+1,\dots,m+n\}\cup\{i\in 1:m\mid a_i\trans \hat{x}=b_i\}$
Then the tangent cone at $\hat{x}$ is $\mathcal{T}(\hat{x}) = \{d\mid a_i\trans d=0, i=m+1,\dots,m+n; a_i\trans d\le0, i\in I(\hat{x})\cap\{1:m\}\}$

One direction is clear by noting the tangent cone contains all feasible direction.
For the other, for $i\in I(\hat{x})$, we have
\[
a_i\trans d = \lim_{k\to\infty}\frac{a_i\trans x_k - a_i\trans\hat{x}}{\alpha_k}=\lim_{k\to\infty}\frac{a_i\trans x_k - b_i}{\alpha_k}
\]
Therefore, for $i\in\{m+1:m+n\}$, we have $a_i\trans d=0$, for $i\in I(\hat{x})\cap\{1:m\}$ we have $a_i\trans d\le0$.
\end{example}

Suppose $\mathcal{X} = \{x\mid h_i(x)=0, i=1:m; g_j(x)\le0, j=1:r\}$ and $I(\hat{x})=\{j\mid g_j(\hat{x})=0\}$.
Introduce the computable cone
\[
\mathcal{C}(\hat{x})=\{d\mid(\nabla h_i(\hat{x}))\trans d = 0,i=1,\dots,m\mid (\nabla g_j(\hat{x}))\trans d\le0, \forall j\in I(\hat{x})\}
\]
Then $\mathcal{T}(\hat{x})\subseteq\mathcal{C}(\hat{x})$
\begin{proposition}
Under Linear Independence Constraint Qualification, i.e., for any $x\in\mathcal{X}$, 
\[
\nabla h_i(x), i=1,\dots,m;\ \nabla g_i(x), j\in I(x)
\]
are always linearly independent.
Together with the Lipschitz continuity of gradient, $\mathcal{T}(\hat{x})=\mathcal{C}(\hat{x})$.
\end{proposition}
\begin{proof}
Ignore equality constraints. By Gordan's theorem, due to LICQ, exists $\hat{d}$ such that $\nabla g_i(\hat{x})\trans\hat{d}<0,i\in I(\hat{x})$. Then for $x(t) = \hat{x}+td+t^{1.5}\hat{d}$, we have $g_i(x(t))<0,i\in I(\hat{x})$, with $t\in(0,\varepsilon)$ and $\varepsilon$ sufficiently small. Therefore, $d\in\mathcal{T}(\hat{x})$.
\end{proof}

\section{Basic Algorithm}
For lipschitz continous $f$, $\min_{x\in\mathcal{L}(x^0,g(x^0),\dots,g(x^{k-1}))}f(x)-f(x^*)\ge\mathcal{O}(1/\sqrt{k})$; for lipschitz continous gradient $f$, $\mathcal{O}(1/k^2)$.
\begin{theorem}
For $0\prec mI\preceq\nabla^2f(x)\preceq MI$, and line search 
\[
t_k = \arg\min_tf(x^k - t\nabla f(x^k)),
\]
we have $f(x^{k+1}) - f(x^*)\le (1-m/M)(f(x^k) - f(x^*))$.
\end{theorem}
\begin{proof}
By MVT, one can show that 
\[
m\|x - x^*\|\le\|\nabla f(x)\|\le M\|x-x^*\|,\quad
f(x+\alpha d)\le f(x) - \frac{1}{2M}\|\nabla f(x)\|^2
\]
Further, $f(x) - f(x^*)\le \|\nabla f(x)\|\|x-x^*\| - \frac{m}{2}\|x-x^*\|^2\le\frac{1}{2m}\|\nabla f(x)\|^2$.

Therefore,
\[
f(x+\alpha d)\le f(x) - \frac{1}{2M}\|\nabla f(x)\|^2\le f(x) - \frac{m}{M}(f(x) - f(x^*))
\]
Or we have the Q-convergence rate
\[
f(x+\alpha d) - f(x^*)\le (1-m/M)(f(x) - f(x^*))
\]
\end{proof}
Dimishing step size:
\[
f(x^{k+1})\le f(x^k) - \frac{\alpha_k}{2}\|\nabla f(x^k)\|^2
\]
Therefore converge. Also, we imply $c\sum_{k}\alpha^k\|\nabla f(x^k)\|^2<\infty$, i.e.,
\begin{align*}
\lim\inf\|\nabla f(x^k)\|\to0&\implies
f(x) - f(x^*)\le\frac{1}{2m}\|\nabla f(x)\|^2\\
&\implies
f(x^{k+1}) - f(x^*)\le (1-m\alpha_k)(f(x^k) - f(x^*)),
\end{align*}
which follows that $\lim f(x^n)\to f(x^*)$ and $\|\nabla f(x^n)\|^2\le 2M(f(x^n) - f(x^*))$.

Armijo's rule:
Fix $s>0,\beta\in(0,1),\gamma\in(0,1)$, and $\ell$ be the smallest integer such that 
\[
f(x^k) - f(x^k+\beta^\ell sd^k)\ge-\gamma s\nabla f(x^k)\trans d^k,
\]
and $\alpha_k=s\beta^\ell$. Therefore,
\begin{align*}
f(x^k)&\ge f(x^k+\alpha_kd^k) - \gamma\alpha_k\nabla f(x^k)\trans d^k\\
f(x^k)&< f(x^k+\alpha_k/\beta d^k) - \gamma\alpha_k/\beta\nabla f(x^k)\trans d^k
\end{align*}
which follows that
\begin{align*}
\alpha_k&>\frac{2\beta(1-\gamma)}{M}\\
f(x^k+\alpha_kd^k)&\le f(x^k)+\gamma\alpha_k\nabla f(x^k)\trans d^k\\
&\le f(x^k)-4\beta\gamma(1-\gamma)\frac{m}{M}(f(x^k) - f(x^*))
\end{align*}
Non-strongly convexity with line-search:
\begin{align*}
f(x+\alpha d)&\le f(x)-\frac{1}{2M}\|\nabla f(x)\|^2\\
f(x) - f(x^*)&\le\|\nabla f(x)\|\|x-x^*\|
\end{align*}
Let $\|x^k - x^*\|$ and $e(x^k) = f(x^k) - f(x^*)$, which follows that
\[
e(x^{k+1})\le e(x^k) - ce(x^k)^2,\quad c = \frac{1}{2MC^2}
\]
which implies
\[
\frac{1}{e(x^{k+1})}\ge\frac{1}{e(x^k)}+\frac{c}{1-ce(x^k)}\ge\frac{1}{e(x^k)}+c\ge\frac{1}{e(x^1)}+kc
\]

Without second order differentiability:
\[
\sigma\|x-y\|^2\le(\nabla f(x) - \nabla f(y))\trans(x-y)\le L\|x-y\|^2
\]
we imply
\[
\frac{\sigma}{2}\|y-x\|^2\le f(y) - f(x) - \nabla f(x)\trans (y-x)\le\frac{L}{2}\|y-x\|^2
\]
and
\[
f(x-\alpha\nabla f(x))-f(x^*)\le (1-\sigma/L)(f(x) - f(x^*))
\]

Local convergence on Newton's method:
\[
x^{k+1} - x^*
=
(\nabla^2f(x^k))^{-1}\left\{
\int_0^1[\nabla^2f(x^k) - \nabla^2f(x^* +t(x^k - x^*))](x^k-x^*)\diff t
\right\}
\]

Least squares: $f(x) = \frac{1}{2}\sum_{i=1}^mf_i(x)^2$, then
\[
\nabla f(x) = \sum_{i=1}^mf_i(x)\nabla f_i(x),\quad
\nabla^2f(x) = \sum_{i=1}^m[\nabla f_i(x)\nabla f_i(x)\trans + f_i(x)\nabla^2f_i(x)]
\]


\section{Interior Point Method}
Remall the primal and dual LP with slater condition holds:
\[
\begin{array}{ll}
\min&c\trans x\\
\text{with}&Ax=b\\
&x\ge0
\end{array},\quad
\begin{array}{ll}
\max&b\trans y\\
&A\trans y+s=c\\
&s\ge0
\end{array}
\]
Define the interior region
\begin{align*}
\mathcal{F}^\circ(P)&=\{x\in\mathbb{R}^n\mid Ax=b,x>0\}\\
\mathcal{F}^\circ(D)&=\{(y,s)\in\mathbb{R}^m\times\mathbb{R}^n\mid A\trans y+s=c,s>0\}
\end{align*}
Consider the problem $(P_\mu):$ $\min_{\mathcal{F}^\circ(P)}B_\mu(x):=c\trans x-\mu\ln x$. By KKT condition,
\begin{itemize}
\item
Suppose $x(\mu)$ is a solution for $(P_\mu)$, and there exists $y(\mu)$ such that 
\[
c - \mu x(\mu)^{-1}-A\trans y(\mu)=0.
\]
Define $s(\mu)=\mu x(\mu)^{-1} = c-A\trans y(\mu)$.
\item
$(y(\mu),s(\mu))$ is a solution to $(D_\mu)$, 
duality gap
$
x(\mu)\trans s(\mu)=n\mu.
$
\item
Boundedness of $x(\mu)$ and $s(\mu)$: $0=(\tilde{x} - x(\mu))\trans(\tilde{s} - s(\mu))$
\item
Limit of central path: suppose as subsequence $\mu_k\to0$, 
\[(x(\mu_k),y(\mu_k),s(\mu_k))\to(\hat{x},\hat{y},\hat{s}).\]
Construct $B=\{i\mid \hat{x}_i>0\}$ and $N=\{j\mid\hat{s}_j>0\}$.
Since $\hat{x}$ primal optimal, $(\hat{y},\hat{s})$ dual optimal, $B\cap N=\emptyset$.
From $0=(\hat{x}-x(\mu_k))\trans(\hat{s} - s(\mu_k))$ we imply
\[
n = \sum_{i\in B}\frac{\hat{x}_i}{x_i(\mu_k)}+\sum_{j\in N}\frac{\hat{s}_j}{s_j(\mu_k)}
\implies
|B|+|N|=n.
\]
Therefore, $B,N$ forms a partition of $\{1:n\}$
\item
Uniqueness of $\hat{x}$: note that $\hat{x}_B$ is unique since it's optimal for the strict concave problem $\{\max\sum_{i\in B}\ln x_i\mid A_Bx_B=b\}$ by verifying KKT condition:
\[
A_Bx_B=b,\
x_B^{-1}\in\text{Range}(A_B\trans)
\]
\end{itemize}


\begin{proposition}
For $B_\mu$ to have a minimizer on $\mathcal{F}^\circ(P)$, it's necessary and sufficient to let $\mathcal{F}^\circ(P)$ and $\mathcal{F}^\circ(D)$ be non-empty
\end{proposition}
\begin{proof}
Sufficiency: In this case, when $(\hat{x},\hat{y},\hat{s})$ is feasible, $B_\mu(x) = \hat{y}\trans b + \sum_{j=1}^n(\hat{s}_jx_j-\mu\ln x_j)$.
Note that when $x_j\to0$ or $x_j\to\infty$, $B_\mu(x)\to\infty$. Therefore, we find $x^{l}_j>0,x^u_j>0$ such that $B_{\mu}(x)\le B_{\mu}(\hat{x})$, we have $0<x^l\le x_j\le x^u,\forall j$.
Since $B_\mu$ is continuous over a non-empty closed and bounded set $\mathcal{C}=\{x\in\mathcal{F}^\circ(P), x^l\le x\le x^u\}$, by Weierstrass theorem, exists a minimizer of $B_{\mu}$ on $\mathcal{C}$.

Necessity: check KKT condition.
\end{proof}
\begin{proposition}
For $\mathcal{F}^\circ(P)$ and $\mathcal{F}^\circ(D)$ be non-empty, a necessary and sufficient condition for $x\in\mathcal{F}^\circ(P)$ be the unique minimizer of $B_\mu$ is that exists $(y,s)\in\mathcal{F}^\circ(D)$ such that
\begin{align*}
A\trans y+s&=c\\
Ax&=b\\
x\circ s&=\mu
\end{align*}
\end{proposition}
\begin{proof}
Consider the KKT condition for the strongly convex problem $\{\min c\trans x - \mu\ln x\mid Ax=b\}$
\end{proof}

\section{Machine Learning}
The solution for $\min~\frac{1}{2}\|\bm A\trans y-\bm b\|^2+\frac{\gamma}{2}\|\bm y\|^2$ is
\[
y^*=(\bm A\bm A\trans+\gamma\bm I)^{-1}\bm A\bm b
\]
Sparse optimization:
\[
\begin{array}{ll}
\min&\rank(X)\\
&X_{ij}=r_{ij},\ \text{known entries $(i,j)\in I$}
\end{array}
\]
Convex relaxation:
\[
\begin{array}{ll}
\min&\|X\|_*\\
&X_{ij}=r_{ij},\ \text{known entries $(i,j)\in I$}
\end{array}
\]
 Or $\min~\|X\|_*+\mu\sum_{(i,j)\in I}(X_{ij}-r_{ij})^2$

Logistic regression model: $\min\sum_{i=1}^m\ln(1+\exp(-s_i(w\trans x_i - b)))$

\section{First-order Methods}
\begin{enumerate}
\item
Conditional gradient: 
\begin{align*}
y^{k+1}&=\arg\min_{y\in\mathcal{X}}(\nabla f(x^k))\trans (y-x^k)\\
x^{k+1}&=x^k+t_k(y^{k+1} - x^k)
\end{align*}
The lipschitz condition for gradient implies
\[
f(y)\le f(x)+(\nabla f(x))\trans(y-x)+\frac{L}{2}\|y-x\|^2
\]
Then we imply
\begin{align*}
f(x^{k+1})&\le f(x^k) + t_k(\nabla f(x^k))\trans(y^{k+1} - x^k)+\frac{L}{2}t_k^2D^2\\
&\le f(x^k)+(f(x^*)-f(x^k))t_k + \frac{LD^2t_k^2}{2}
\end{align*}
\item
Proximal point algorithm: $x^{k+1} = \arg\min_{x\in\mathcal{X}}f_k(x)+\frac{1}{2t_k}\|x-x^k\|^2$, where $f_k$ is approximation of $f$.

The subgradient inequality is that $f(y)\ge f(x)+\nabla f(x)\trans(y-x)$. For iteration $\ell$,
\begin{align*}
\left(d^{\ell+1}-\frac{x^\ell-x^{\ell+1}}{t_{\ell}}\right)\trans(x-x^{\ell+1})&\ge0,\forall x\in\mathcal{X}\\
f_\ell(x) - f_\ell(x^{\ell+1})&\ge\left(\frac{x^\ell-x^{\ell+1}}{t_{\ell}}\right)\trans(x-x^{\ell+1})\\
\|x^{\ell}-x\|^2\ge\|x^{\ell+1}-x\|^2+&2t_{\ell}(f_{\ell}(x^{\ell+1}) - f_\ell(x))+\|x^{\ell+1}-x^\ell\|^2\\
\|x^{\ell+1}-x^\ell\|^2\le\|x^{\ell}-x\|^2 &- 2t_{\ell}(f_{\ell}(x^{\ell+1}) - f_\ell(x))-\|x^{\ell+1}-x^\ell\|^2
\end{align*}

The proximal point algorithm yields monotonically improving iterates:
\[
f(x^{\ell+1})\le f_{\ell}(x^{\ell+1})+\frac{L}{2}\|x^{\ell+1}-x^{\ell}\|^2\le f_\ell(x^\ell)=f(x^\ell)
\]
\item
Gradient Projection: $x^{k+1} = [x^k - t_k\nabla f(x^k)]_{\mathcal{X}}$, i.e., 
\[
x^{k+1} = \arg\min_{x\in\mathcal{X}}(\nabla f(x^k))\trans(x-x^{\ell})+\frac{1}{2t_\ell}\|x-x^\ell\|^2
\]
This is because
\[
(d^{\ell})\trans(x-x^{\ell})+\frac{1}{2t_\ell}\|x-x^\ell\|^2=\frac{1}{2t_\ell}\|x-(x^\ell-t_\ell d^\ell)\|^2 - \frac{t_\ell}{2}\|d^\ell\|^2
\]
\item
ISTA: 
\[
x^{k+1} = \arg\min_{x\in\mathcal{X}}\nabla f(x^k)\trans(x-x^k)+\frac{L}{2}\|x-x^k\|^2+h(x)
\]
Apply the optimality condition,
\begin{align*}
h(x^{\ell+1})&\le h(x)-h'(x^{\ell+1})\trans(x-x^{\ell+1})\\
&\le h(x)+\nabla f(x^{\ell})\trans(x-x^{\ell+1})+L(x^{\ell+1}-x^{\ell})\trans(x-x^{\ell+1})
\end{align*}
Then bound $f(x^{\ell+1})$:
\[
f(x^{\ell+1})\le f(x)+\nabla f(x^{\ell})\trans(x^{\ell+1}-x)+\frac{L}{2}\|x^{\ell+1}-x^\ell\|^2
\]
Then we bound $h(x^{\ell+1})+f(x^{\ell+1})$, by using
\[
2(w_1-w_2)\trans(w_3-w_1)+\|w_1-w_3\|^2=\|w_2-w_3\|^2-\|w_1-w_2\|^2
\]
\item
Bregman Distance: $B(y,x) = \Phi(y)-\Phi(x)-\nabla\Phi(x)\trans(y-x)$, where $\Phi$ is smooth and strongly convex.
\[
x^{k+1} = \arg\min_{x\in\mathcal{X}}(d^k)\trans(x-x^k)+\frac{1}{t_k}B(x,x^k),\ d^k\in\partial f(x^k).
\]
Observe $B(y,x)\ge\sigma\|y-x\|$ and
\[
B(w,u)+B(u,v)-B(w,v)=[\nabla \Phi(v) - \Phi(u)]\trans(w-u)
\]
Then bound
\[
t^{\ell}(d^{\ell})\trans(x^*-x^{\ell})\ge B(x^*,x^{\ell+1}) - B(x^*,x^{\ell})+\sigma\|x^\ell-x^{\ell+1}\|^2
\]
and
\begin{align*}
t^{\ell}(f(x^*)-f(x^{\ell}))&\ge B(x^*,x^{\ell+1}) - B(x^*,x^{\ell})+\sigma\|x^\ell-x^{\ell+1}\|^2 - \\&\left(
\frac{\tau}{2}\|x^\ell-x^{\ell+1}\|^2+\frac{t_\ell^2}{2\tau}\|d^\ell\|^2
\right)
,\forall\tau
\end{align*}
Let $\tau=2\sigma$ and $t_\ell=1/\sqrt{k}$.

For Lipschitz continuity, bound $t_\ell(d^{\ell})\trans(x^{\ell+1}-x^\ell)$.
\end{enumerate}




























